{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a03009",
   "metadata": {},
   "source": [
    "# Project: MVCNN for Classification and Reconstruction from Multi-views 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa52c63",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50bb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import k3d\n",
    "import trimesh\n",
    "import torch\n",
    "import skimage\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decb2093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillow Version: 7.2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import PIL\n",
    "print('Pillow Version:', PIL.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f4e16",
   "metadata": {},
   "source": [
    "## 0. Starting up (1 time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e3d29",
   "metadata": {},
   "source": [
    "### (a) Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Downloading ...')\n",
    "# File sizes: 11GB for ShapeNetRendering.tgz (Multiview images), 22MB for ShapeNetVox32.tgz (target voxels)\n",
    "!mkdir -p datasets\n",
    "!wget http://cvgl.stanford.edu/data2/ShapeNetRendering.tgz -P datasets\n",
    "!wget http://cvgl.stanford.edu/data2/ShapeNetVox32.tgz -P datasets\n",
    "print('Extracting ...')\n",
    "!tar -xvf datasets/ShapeNetRendering.tgz -d datasets\n",
    "!tar -xvf datasets/ShapeNetVox32.tgz -d datasets\n",
    "!rm datasets/ShapeNetRendering.tgz\n",
    "!rm datasets/ShapeNetVox32.tgz\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088613cd",
   "metadata": {},
   "source": [
    "### (b) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e14cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 30642\n",
      "Length of val set: 4371\n"
     ]
    }
   ],
   "source": [
    "from mvcnn_rec.data.shapenet import ShapeNetMultiview\n",
    "\n",
    "train_dataset = ShapeNetMultiview('train', total_views=24, num_views=12, \n",
    "                                  load_mode='mvcnn_rec', # Change to mvcnn to get only images and labels\n",
    "                                  random_start_view=False)\n",
    "val_dataset = ShapeNetMultiview('val', total_views=24, num_views=12, \n",
    "                                load_mode='mvcnn_rec', # Change to mvcnn to get only images and labels\n",
    "                                random_start_view=False)\n",
    "\n",
    "print(f'Length of train set: {len(train_dataset)}') # Expected: 30642\n",
    "print(f'Length of val set: {len(val_dataset)}') # Expected: 4371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "587bcf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "train_sample = train_dataset[0] \n",
    "print(train_sample[\"item\"].shape) # Expected torch.Size([12, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "983954bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 02691156/70e4200e848e653072ec6e905035e5d7\n",
      "Voxel Dimensions: (1, 32, 32, 32)\n",
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb5c270e1a9487a9f4d7dd76cb52e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some shapes\n",
    "from mvcnn_rec.util.visualization import visualize_occupancy\n",
    "\n",
    "print(f'Name: {train_sample[\"name\"]}')\n",
    "print(f'Voxel Dimensions: {train_sample[\"voxel\"].shape}')\n",
    "print(f'Label: {train_sample[\"label\"]}')\n",
    "\n",
    "visualize_occupancy(train_sample[\"voxel\"].squeeze(), flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b3595",
   "metadata": {},
   "source": [
    "### (c) Data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "544c85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "        batch_size=4,   # The size of batches is defined here\n",
    "        shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "        num_workers=4,   # Data is usually loaded in parallel by num_workers\n",
    "        pin_memory=True  # This is an implementation detail to speed up data uploading to the GPU\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72fdd6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "    input_data, target_labels = batch['item'], batch['label']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6faf954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 12, 3, 224, 224]), torch.Size([4]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape, target_labels.shape # Expected (torch.Size([4, 12, 3, 224, 224]), torch.Size([4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31c3a014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 4, 3, 224, 224])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N,B,C,H,W = input_data.size()\n",
    "input_data = input_data.view(B, N, C, H, W)\n",
    "input_data.shape # Expected (torch.Size([12, 4, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c58860",
   "metadata": {},
   "source": [
    "## 1. Multiview CNN for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed60f10",
   "metadata": {},
   "source": [
    "### (a) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1083da37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name                           | Type              | Params  \n",
      "------------------------------------------------------------------------\n",
      "0  | encoder_image                  | Sequential        | 11176512\n",
      "1  | encoder_image.0                | Conv2d            | 9408    \n",
      "2  | encoder_image.1                | BatchNorm2d       | 128     \n",
      "3  | encoder_image.2                | ReLU              | 0       \n",
      "4  | encoder_image.3                | MaxPool2d         | 0       \n",
      "5  | encoder_image.4                | Sequential        | 147968  \n",
      "6  | encoder_image.4.0              | BasicBlock        | 73984   \n",
      "7  | encoder_image.4.0.conv1        | Conv2d            | 36864   \n",
      "8  | encoder_image.4.0.bn1          | BatchNorm2d       | 128     \n",
      "9  | encoder_image.4.0.relu         | ReLU              | 0       \n",
      "10 | encoder_image.4.0.conv2        | Conv2d            | 36864   \n",
      "11 | encoder_image.4.0.bn2          | BatchNorm2d       | 128     \n",
      "12 | encoder_image.4.1              | BasicBlock        | 73984   \n",
      "13 | encoder_image.4.1.conv1        | Conv2d            | 36864   \n",
      "14 | encoder_image.4.1.bn1          | BatchNorm2d       | 128     \n",
      "15 | encoder_image.4.1.relu         | ReLU              | 0       \n",
      "16 | encoder_image.4.1.conv2        | Conv2d            | 36864   \n",
      "17 | encoder_image.4.1.bn2          | BatchNorm2d       | 128     \n",
      "18 | encoder_image.5                | Sequential        | 525568  \n",
      "19 | encoder_image.5.0              | BasicBlock        | 230144  \n",
      "20 | encoder_image.5.0.conv1        | Conv2d            | 73728   \n",
      "21 | encoder_image.5.0.bn1          | BatchNorm2d       | 256     \n",
      "22 | encoder_image.5.0.relu         | ReLU              | 0       \n",
      "23 | encoder_image.5.0.conv2        | Conv2d            | 147456  \n",
      "24 | encoder_image.5.0.bn2          | BatchNorm2d       | 256     \n",
      "25 | encoder_image.5.0.downsample   | Sequential        | 8448    \n",
      "26 | encoder_image.5.0.downsample.0 | Conv2d            | 8192    \n",
      "27 | encoder_image.5.0.downsample.1 | BatchNorm2d       | 256     \n",
      "28 | encoder_image.5.1              | BasicBlock        | 295424  \n",
      "29 | encoder_image.5.1.conv1        | Conv2d            | 147456  \n",
      "30 | encoder_image.5.1.bn1          | BatchNorm2d       | 256     \n",
      "31 | encoder_image.5.1.relu         | ReLU              | 0       \n",
      "32 | encoder_image.5.1.conv2        | Conv2d            | 147456  \n",
      "33 | encoder_image.5.1.bn2          | BatchNorm2d       | 256     \n",
      "34 | encoder_image.6                | Sequential        | 2099712 \n",
      "35 | encoder_image.6.0              | BasicBlock        | 919040  \n",
      "36 | encoder_image.6.0.conv1        | Conv2d            | 294912  \n",
      "37 | encoder_image.6.0.bn1          | BatchNorm2d       | 512     \n",
      "38 | encoder_image.6.0.relu         | ReLU              | 0       \n",
      "39 | encoder_image.6.0.conv2        | Conv2d            | 589824  \n",
      "40 | encoder_image.6.0.bn2          | BatchNorm2d       | 512     \n",
      "41 | encoder_image.6.0.downsample   | Sequential        | 33280   \n",
      "42 | encoder_image.6.0.downsample.0 | Conv2d            | 32768   \n",
      "43 | encoder_image.6.0.downsample.1 | BatchNorm2d       | 512     \n",
      "44 | encoder_image.6.1              | BasicBlock        | 1180672 \n",
      "45 | encoder_image.6.1.conv1        | Conv2d            | 589824  \n",
      "46 | encoder_image.6.1.bn1          | BatchNorm2d       | 512     \n",
      "47 | encoder_image.6.1.relu         | ReLU              | 0       \n",
      "48 | encoder_image.6.1.conv2        | Conv2d            | 589824  \n",
      "49 | encoder_image.6.1.bn2          | BatchNorm2d       | 512     \n",
      "50 | encoder_image.7                | Sequential        | 8393728 \n",
      "51 | encoder_image.7.0              | BasicBlock        | 3673088 \n",
      "52 | encoder_image.7.0.conv1        | Conv2d            | 1179648 \n",
      "53 | encoder_image.7.0.bn1          | BatchNorm2d       | 1024    \n",
      "54 | encoder_image.7.0.relu         | ReLU              | 0       \n",
      "55 | encoder_image.7.0.conv2        | Conv2d            | 2359296 \n",
      "56 | encoder_image.7.0.bn2          | BatchNorm2d       | 1024    \n",
      "57 | encoder_image.7.0.downsample   | Sequential        | 132096  \n",
      "58 | encoder_image.7.0.downsample.0 | Conv2d            | 131072  \n",
      "59 | encoder_image.7.0.downsample.1 | BatchNorm2d       | 1024    \n",
      "60 | encoder_image.7.1              | BasicBlock        | 4720640 \n",
      "61 | encoder_image.7.1.conv1        | Conv2d            | 2359296 \n",
      "62 | encoder_image.7.1.bn1          | BatchNorm2d       | 1024    \n",
      "63 | encoder_image.7.1.relu         | ReLU              | 0       \n",
      "64 | encoder_image.7.1.conv2        | Conv2d            | 2359296 \n",
      "65 | encoder_image.7.1.bn2          | BatchNorm2d       | 1024    \n",
      "66 | encoder_image.8                | AdaptiveAvgPool2d | 0       \n",
      "67 | classifier                     | Sequential        | 6669    \n",
      "68 | classifier.0                   | Linear            | 6669    \n",
      "69 | TOTAL                          | MVCNN             | 11183181\n"
     ]
    }
   ],
   "source": [
    "from mvcnn_rec.model.mvcnn import MVCNN\n",
    "from mvcnn_rec.util.model import summarize_model\n",
    "\n",
    "mvcnn = MVCNN(13)\n",
    "print(summarize_model(mvcnn))  # Expected: Rows 0-68 and TOTAL = 11183181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2ec6714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape:  torch.Size([8, 13])\n"
     ]
    }
   ],
   "source": [
    "multi_images = torch.randn(12, 8, 3, 224, 224) * 2. - 1. # Suppose 12 images per shape, 8 shapes\n",
    "pred_classes = mvcnn(multi_images)\n",
    "print('Output tensor shape: ', pred_classes.shape)  # Expected: torch.Size([8, 13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db3ef8",
   "metadata": {},
   "source": [
    "### (b) Training script and overfit one shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba2f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[004/00000] train_loss: 1.056\n",
      "[009/00000] train_loss: 0.012\n",
      "[009/00000] val_loss: 0.000, val_accuracy: 100.000%\n",
      "[014/00000] train_loss: 0.001\n",
      "[019/00000] train_loss: 0.000\n",
      "[019/00000] val_loss: 0.000, val_accuracy: 100.000%\n",
      "[024/00000] train_loss: 0.000\n",
      "[029/00000] train_loss: 0.000\n",
      "[029/00000] val_loss: 0.000, val_accuracy: 100.000%\n",
      "[034/00000] train_loss: 0.000\n",
      "[039/00000] train_loss: 0.000\n",
      "[039/00000] val_loss: 0.000, val_accuracy: 100.000%\n",
      "[044/00000] train_loss: 0.000\n",
      "[049/00000] train_loss: 0.000\n",
      "[049/00000] val_loss: 0.000, val_accuracy: 100.000%\n"
     ]
    }
   ],
   "source": [
    "from mvcnn_rec.training import train_mvcnn\n",
    "config = {\n",
    "    'experiment_name': 'mvcnn_overfitting',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 50,\n",
    "    'print_every_n': 5,\n",
    "    'validate_every_n': 10,\n",
    "    'num_views': 8,\n",
    "}\n",
    "train_mvcnn.main(config)  # should be able to get ~0.0 train_loss and ~0.0 val_loss and 100% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceec90d",
   "metadata": {},
   "source": [
    "### (b) Finetune one 1 image view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76353a32",
   "metadata": {},
   "source": [
    "#### FREEZEE backbone. Finetune only FC layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aade37eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'experiment_name': '1vcnn_generalize',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 30,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 100,\n",
    "    'num_views': 1, # Num views from total 24 views (stride = 24 / num_views)\n",
    "    'random_start_view': True, # Set to False to get views start from idx 0. Otherwise random from [0, stride)\n",
    "    'freezee_backbone': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57728034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 1.720\n",
      "[000/00099] train_loss: 1.152\n",
      "[000/00099] val_loss: 1.008, val_accuracy: 71.517%\n",
      "[000/00149] train_loss: 0.960\n",
      "[000/00199] train_loss: 0.855\n",
      "[000/00199] val_loss: 0.803, val_accuracy: 75.635%\n",
      "[000/00249] train_loss: 0.800\n",
      "[000/00299] train_loss: 0.750\n",
      "[000/00299] val_loss: 0.709, val_accuracy: 78.655%\n",
      "[000/00349] train_loss: 0.708\n",
      "[000/00399] train_loss: 0.703\n",
      "[000/00399] val_loss: 0.684, val_accuracy: 78.540%\n",
      "[000/00449] train_loss: 0.679\n",
      "[001/00020] train_loss: 0.684\n",
      "[001/00020] val_loss: 0.641, val_accuracy: 80.096%\n",
      "[001/00070] train_loss: 0.644\n",
      "[001/00120] train_loss: 0.630\n",
      "[001/00120] val_loss: 0.625, val_accuracy: 80.622%\n",
      "[001/00170] train_loss: 0.642\n",
      "[001/00220] train_loss: 0.632\n",
      "[001/00220] val_loss: 0.609, val_accuracy: 80.965%\n",
      "[001/00270] train_loss: 0.596\n",
      "[001/00320] train_loss: 0.612\n",
      "[001/00320] val_loss: 0.601, val_accuracy: 81.720%\n",
      "[001/00370] train_loss: 0.613\n",
      "[001/00420] train_loss: 0.615\n",
      "[001/00420] val_loss: 0.603, val_accuracy: 80.782%\n",
      "[001/00470] train_loss: 0.611\n",
      "[002/00041] train_loss: 0.581\n",
      "[002/00041] val_loss: 0.576, val_accuracy: 82.064%\n",
      "[002/00091] train_loss: 0.579\n",
      "[002/00141] train_loss: 0.595\n",
      "[002/00141] val_loss: 0.562, val_accuracy: 82.178%\n",
      "[002/00191] train_loss: 0.590\n",
      "[002/00241] train_loss: 0.555\n",
      "[002/00241] val_loss: 0.564, val_accuracy: 82.361%\n",
      "[002/00291] train_loss: 0.601\n",
      "[002/00341] train_loss: 0.575\n",
      "[002/00341] val_loss: 0.552, val_accuracy: 82.361%\n",
      "[002/00391] train_loss: 0.581\n",
      "[002/00441] train_loss: 0.565\n",
      "[002/00441] val_loss: 0.568, val_accuracy: 82.109%\n",
      "[003/00012] train_loss: 0.550\n",
      "[003/00062] train_loss: 0.545\n",
      "[003/00062] val_loss: 0.556, val_accuracy: 82.292%\n",
      "[003/00112] train_loss: 0.578\n",
      "[003/00162] train_loss: 0.535\n",
      "[003/00162] val_loss: 0.535, val_accuracy: 82.727%\n",
      "[003/00212] train_loss: 0.568\n",
      "[003/00262] train_loss: 0.563\n",
      "[003/00262] val_loss: 0.546, val_accuracy: 82.384%\n",
      "[003/00312] train_loss: 0.571\n",
      "[003/00362] train_loss: 0.550\n",
      "[003/00362] val_loss: 0.543, val_accuracy: 82.979%\n",
      "[003/00412] train_loss: 0.560\n",
      "[003/00462] train_loss: 0.540\n",
      "[003/00462] val_loss: 0.523, val_accuracy: 83.047%\n",
      "[004/00033] train_loss: 0.538\n",
      "[004/00083] train_loss: 0.520\n",
      "[004/00083] val_loss: 0.533, val_accuracy: 83.116%\n",
      "[004/00133] train_loss: 0.545\n",
      "[004/00183] train_loss: 0.584\n",
      "[004/00183] val_loss: 0.530, val_accuracy: 83.070%\n",
      "[004/00233] train_loss: 0.568\n",
      "[004/00283] train_loss: 0.539\n",
      "[004/00283] val_loss: 0.529, val_accuracy: 82.979%\n",
      "[004/00333] train_loss: 0.510\n",
      "[004/00383] train_loss: 0.541\n",
      "[004/00383] val_loss: 0.523, val_accuracy: 83.413%\n",
      "[004/00433] train_loss: 0.547\n",
      "[005/00004] train_loss: 0.515\n",
      "[005/00004] val_loss: 0.537, val_accuracy: 83.047%\n",
      "[005/00054] train_loss: 0.541\n",
      "[005/00104] train_loss: 0.558\n",
      "[005/00104] val_loss: 0.513, val_accuracy: 83.436%\n",
      "[005/00154] train_loss: 0.532\n",
      "[005/00204] train_loss: 0.562\n",
      "[005/00204] val_loss: 0.519, val_accuracy: 83.345%\n",
      "[005/00254] train_loss: 0.523\n",
      "[005/00304] train_loss: 0.528\n",
      "[005/00304] val_loss: 0.514, val_accuracy: 83.642%\n",
      "[005/00354] train_loss: 0.522\n",
      "[005/00404] train_loss: 0.554\n",
      "[005/00404] val_loss: 0.542, val_accuracy: 82.956%\n",
      "[005/00454] train_loss: 0.487\n",
      "[006/00025] train_loss: 0.520\n",
      "[006/00025] val_loss: 0.514, val_accuracy: 83.688%\n",
      "[006/00075] train_loss: 0.509\n",
      "[006/00125] train_loss: 0.517\n",
      "[006/00125] val_loss: 0.514, val_accuracy: 83.574%\n",
      "[006/00175] train_loss: 0.537\n",
      "[006/00225] train_loss: 0.532\n",
      "[006/00225] val_loss: 0.509, val_accuracy: 83.894%\n",
      "[006/00275] train_loss: 0.539\n",
      "[006/00325] train_loss: 0.525\n",
      "[006/00325] val_loss: 0.513, val_accuracy: 84.031%\n",
      "[006/00375] train_loss: 0.521\n",
      "[006/00425] train_loss: 0.545\n",
      "[006/00425] val_loss: 0.509, val_accuracy: 83.917%\n",
      "[006/00475] train_loss: 0.532\n",
      "[007/00046] train_loss: 0.548\n",
      "[007/00046] val_loss: 0.519, val_accuracy: 83.345%\n",
      "[007/00096] train_loss: 0.537\n",
      "[007/00146] train_loss: 0.551\n",
      "[007/00146] val_loss: 0.519, val_accuracy: 83.596%\n",
      "[007/00196] train_loss: 0.478\n",
      "[007/00246] train_loss: 0.505\n",
      "[007/00246] val_loss: 0.506, val_accuracy: 83.985%\n",
      "[007/00296] train_loss: 0.513\n",
      "[007/00346] train_loss: 0.538\n",
      "[007/00346] val_loss: 0.514, val_accuracy: 83.757%\n",
      "[007/00396] train_loss: 0.539\n",
      "[007/00446] train_loss: 0.514\n",
      "[007/00446] val_loss: 0.503, val_accuracy: 83.871%\n",
      "[008/00017] train_loss: 0.496\n",
      "[008/00067] train_loss: 0.515\n",
      "[008/00067] val_loss: 0.507, val_accuracy: 83.962%\n",
      "[008/00117] train_loss: 0.522\n",
      "[008/00167] train_loss: 0.508\n",
      "[008/00167] val_loss: 0.508, val_accuracy: 83.322%\n",
      "[008/00217] train_loss: 0.499\n",
      "[008/00267] train_loss: 0.499\n",
      "[008/00267] val_loss: 0.519, val_accuracy: 83.070%\n",
      "[008/00317] train_loss: 0.510\n",
      "[008/00367] train_loss: 0.561\n",
      "[008/00367] val_loss: 0.524, val_accuracy: 83.253%\n",
      "[008/00417] train_loss: 0.508\n",
      "[008/00467] train_loss: 0.501\n",
      "[008/00467] val_loss: 0.499, val_accuracy: 84.054%\n",
      "[009/00038] train_loss: 0.497\n",
      "[009/00088] train_loss: 0.524\n",
      "[009/00088] val_loss: 0.498, val_accuracy: 84.283%\n",
      "[009/00138] train_loss: 0.511\n",
      "[009/00188] train_loss: 0.505\n",
      "[009/00188] val_loss: 0.496, val_accuracy: 84.031%\n",
      "[009/00238] train_loss: 0.517\n",
      "[009/00288] train_loss: 0.529\n",
      "[009/00288] val_loss: 0.514, val_accuracy: 83.711%\n",
      "[009/00338] train_loss: 0.527\n",
      "[009/00388] train_loss: 0.527\n",
      "[009/00388] val_loss: 0.493, val_accuracy: 84.329%\n",
      "[009/00438] train_loss: 0.494\n",
      "[010/00009] train_loss: 0.485\n",
      "[010/00009] val_loss: 0.506, val_accuracy: 84.214%\n",
      "[010/00059] train_loss: 0.511\n",
      "[010/00109] train_loss: 0.511\n",
      "[010/00109] val_loss: 0.492, val_accuracy: 84.100%\n",
      "[010/00159] train_loss: 0.522\n",
      "[010/00209] train_loss: 0.537\n",
      "[010/00209] val_loss: 0.504, val_accuracy: 84.168%\n",
      "[010/00259] train_loss: 0.527\n",
      "[010/00309] train_loss: 0.495\n",
      "[010/00309] val_loss: 0.535, val_accuracy: 82.430%\n",
      "[010/00359] train_loss: 0.520\n",
      "[010/00409] train_loss: 0.513\n",
      "[010/00409] val_loss: 0.524, val_accuracy: 83.482%\n",
      "[010/00459] train_loss: 0.514\n",
      "[011/00030] train_loss: 0.479\n",
      "[011/00030] val_loss: 0.493, val_accuracy: 84.374%\n",
      "[011/00080] train_loss: 0.506\n",
      "[011/00130] train_loss: 0.506\n",
      "[011/00130] val_loss: 0.502, val_accuracy: 83.688%\n",
      "[011/00180] train_loss: 0.514\n",
      "[011/00230] train_loss: 0.520\n",
      "[011/00230] val_loss: 0.515, val_accuracy: 83.482%\n",
      "[011/00280] train_loss: 0.486\n",
      "[011/00330] train_loss: 0.549\n",
      "[011/00330] val_loss: 0.514, val_accuracy: 83.299%\n",
      "[011/00380] train_loss: 0.516\n",
      "[011/00430] train_loss: 0.532\n",
      "[011/00430] val_loss: 0.510, val_accuracy: 83.734%\n",
      "[012/00001] train_loss: 0.506\n",
      "[012/00051] train_loss: 0.501\n",
      "[012/00051] val_loss: 0.497, val_accuracy: 83.734%\n",
      "[012/00101] train_loss: 0.488\n",
      "[012/00151] train_loss: 0.522\n",
      "[012/00151] val_loss: 0.507, val_accuracy: 83.802%\n",
      "[012/00201] train_loss: 0.524\n",
      "[012/00251] train_loss: 0.508\n",
      "[012/00251] val_loss: 0.488, val_accuracy: 84.146%\n",
      "[012/00301] train_loss: 0.502\n",
      "[012/00351] train_loss: 0.516\n",
      "[012/00351] val_loss: 0.489, val_accuracy: 84.329%\n",
      "[012/00401] train_loss: 0.519\n",
      "[012/00451] train_loss: 0.528\n",
      "[012/00451] val_loss: 0.485, val_accuracy: 84.168%\n",
      "[013/00022] train_loss: 0.469\n",
      "[013/00072] train_loss: 0.494\n",
      "[013/00072] val_loss: 0.497, val_accuracy: 84.054%\n",
      "[013/00122] train_loss: 0.502\n",
      "[013/00172] train_loss: 0.519\n",
      "[013/00172] val_loss: 0.486, val_accuracy: 84.717%\n",
      "[013/00222] train_loss: 0.515\n",
      "[013/00272] train_loss: 0.521\n",
      "[013/00272] val_loss: 0.494, val_accuracy: 84.054%\n",
      "[013/00322] train_loss: 0.501\n",
      "[013/00372] train_loss: 0.501\n",
      "[013/00372] val_loss: 0.492, val_accuracy: 83.940%\n",
      "[013/00422] train_loss: 0.516\n",
      "[013/00472] train_loss: 0.526\n",
      "[013/00472] val_loss: 0.488, val_accuracy: 84.351%\n",
      "[014/00043] train_loss: 0.505\n",
      "[014/00093] train_loss: 0.516\n",
      "[014/00093] val_loss: 0.496, val_accuracy: 84.100%\n",
      "[014/00143] train_loss: 0.505\n",
      "[014/00193] train_loss: 0.508\n",
      "[014/00193] val_loss: 0.483, val_accuracy: 84.832%\n",
      "[014/00243] train_loss: 0.488\n",
      "[014/00293] train_loss: 0.509\n",
      "[014/00293] val_loss: 0.502, val_accuracy: 83.962%\n",
      "[014/00343] train_loss: 0.518\n",
      "[014/00393] train_loss: 0.469\n",
      "[014/00393] val_loss: 0.494, val_accuracy: 84.512%\n",
      "[014/00443] train_loss: 0.488\n",
      "[015/00014] train_loss: 0.511\n",
      "[015/00014] val_loss: 0.501, val_accuracy: 83.734%\n",
      "[015/00064] train_loss: 0.512\n",
      "[015/00114] train_loss: 0.495\n",
      "[015/00114] val_loss: 0.505, val_accuracy: 83.688%\n",
      "[015/00164] train_loss: 0.503\n",
      "[015/00214] train_loss: 0.465\n",
      "[015/00214] val_loss: 0.485, val_accuracy: 84.603%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[015/00264] train_loss: 0.499\n",
      "[015/00314] train_loss: 0.527\n",
      "[015/00314] val_loss: 0.498, val_accuracy: 84.031%\n",
      "[015/00364] train_loss: 0.526\n",
      "[015/00414] train_loss: 0.468\n",
      "[015/00414] val_loss: 0.490, val_accuracy: 84.146%\n",
      "[015/00464] train_loss: 0.505\n",
      "[016/00035] train_loss: 0.494\n",
      "[016/00035] val_loss: 0.505, val_accuracy: 84.008%\n",
      "[016/00085] train_loss: 0.516\n",
      "[016/00135] train_loss: 0.513\n",
      "[016/00135] val_loss: 0.497, val_accuracy: 83.734%\n",
      "[016/00185] train_loss: 0.510\n",
      "[016/00235] train_loss: 0.474\n",
      "[016/00235] val_loss: 0.487, val_accuracy: 84.512%\n",
      "[016/00285] train_loss: 0.500\n",
      "[016/00335] train_loss: 0.503\n",
      "[016/00335] val_loss: 0.491, val_accuracy: 84.283%\n",
      "[016/00385] train_loss: 0.492\n",
      "[016/00435] train_loss: 0.507\n",
      "[016/00435] val_loss: 0.500, val_accuracy: 84.031%\n",
      "[017/00006] train_loss: 0.495\n",
      "[017/00056] train_loss: 0.506\n",
      "[017/00056] val_loss: 0.486, val_accuracy: 84.237%\n",
      "[017/00106] train_loss: 0.520\n",
      "[017/00156] train_loss: 0.505\n",
      "[017/00156] val_loss: 0.492, val_accuracy: 84.214%\n",
      "[017/00206] train_loss: 0.495\n",
      "[017/00256] train_loss: 0.499\n",
      "[017/00256] val_loss: 0.526, val_accuracy: 83.391%\n",
      "[017/00306] train_loss: 0.521\n",
      "[017/00356] train_loss: 0.486\n",
      "[017/00356] val_loss: 0.501, val_accuracy: 84.191%\n",
      "[017/00406] train_loss: 0.509\n",
      "[017/00456] train_loss: 0.519\n",
      "[017/00456] val_loss: 0.483, val_accuracy: 84.397%\n",
      "[018/00027] train_loss: 0.490\n",
      "[018/00077] train_loss: 0.492\n",
      "[018/00077] val_loss: 0.488, val_accuracy: 84.306%\n",
      "[018/00127] train_loss: 0.492\n",
      "[018/00177] train_loss: 0.523\n",
      "[018/00177] val_loss: 0.486, val_accuracy: 84.214%\n",
      "[018/00227] train_loss: 0.517\n",
      "[018/00277] train_loss: 0.480\n",
      "[018/00277] val_loss: 0.488, val_accuracy: 84.717%\n",
      "[018/00327] train_loss: 0.491\n",
      "[018/00377] train_loss: 0.501\n",
      "[018/00377] val_loss: 0.491, val_accuracy: 84.077%\n",
      "[018/00427] train_loss: 0.523\n",
      "[018/00477] train_loss: 0.463\n",
      "[018/00477] val_loss: 0.506, val_accuracy: 83.665%\n",
      "[019/00048] train_loss: 0.484\n",
      "[019/00098] train_loss: 0.498\n",
      "[019/00098] val_loss: 0.487, val_accuracy: 84.031%\n",
      "[019/00148] train_loss: 0.478\n",
      "[019/00198] train_loss: 0.480\n",
      "[019/00198] val_loss: 0.484, val_accuracy: 84.443%\n",
      "[019/00248] train_loss: 0.483\n",
      "[019/00298] train_loss: 0.488\n",
      "[019/00298] val_loss: 0.480, val_accuracy: 84.786%\n",
      "[019/00348] train_loss: 0.476\n",
      "[019/00398] train_loss: 0.508\n",
      "[019/00398] val_loss: 0.518, val_accuracy: 83.619%\n",
      "[019/00448] train_loss: 0.501\n",
      "[020/00019] train_loss: 0.466\n",
      "[020/00019] val_loss: 0.497, val_accuracy: 84.260%\n",
      "[020/00069] train_loss: 0.485\n",
      "[020/00119] train_loss: 0.516\n",
      "[020/00119] val_loss: 0.485, val_accuracy: 84.374%\n",
      "[020/00169] train_loss: 0.513\n",
      "[020/00219] train_loss: 0.493\n",
      "[020/00219] val_loss: 0.489, val_accuracy: 84.489%\n",
      "[020/00269] train_loss: 0.486\n",
      "[020/00319] train_loss: 0.488\n",
      "[020/00319] val_loss: 0.487, val_accuracy: 84.191%\n",
      "[020/00369] train_loss: 0.535\n",
      "[020/00419] train_loss: 0.466\n",
      "[020/00419] val_loss: 0.494, val_accuracy: 84.077%\n",
      "[020/00469] train_loss: 0.494\n",
      "[021/00040] train_loss: 0.497\n",
      "[021/00040] val_loss: 0.478, val_accuracy: 84.420%\n",
      "[021/00090] train_loss: 0.486\n",
      "[021/00140] train_loss: 0.498\n",
      "[021/00140] val_loss: 0.488, val_accuracy: 84.397%\n",
      "[021/00190] train_loss: 0.478\n",
      "[021/00240] train_loss: 0.493\n",
      "[021/00240] val_loss: 0.500, val_accuracy: 84.008%\n",
      "[021/00290] train_loss: 0.543\n",
      "[021/00340] train_loss: 0.500\n",
      "[021/00340] val_loss: 0.514, val_accuracy: 83.551%\n",
      "[021/00390] train_loss: 0.480\n",
      "[021/00440] train_loss: 0.478\n",
      "[021/00440] val_loss: 0.477, val_accuracy: 84.603%\n",
      "[022/00011] train_loss: 0.492\n",
      "[022/00061] train_loss: 0.496\n",
      "[022/00061] val_loss: 0.513, val_accuracy: 83.185%\n",
      "[022/00111] train_loss: 0.511\n",
      "[022/00161] train_loss: 0.484\n",
      "[022/00161] val_loss: 0.490, val_accuracy: 84.283%\n",
      "[022/00211] train_loss: 0.468\n",
      "[022/00261] train_loss: 0.493\n",
      "[022/00261] val_loss: 0.476, val_accuracy: 84.717%\n",
      "[022/00311] train_loss: 0.501\n",
      "[022/00361] train_loss: 0.490\n",
      "[022/00361] val_loss: 0.501, val_accuracy: 83.596%\n",
      "[022/00411] train_loss: 0.535\n",
      "[022/00461] train_loss: 0.511\n",
      "[022/00461] val_loss: 0.486, val_accuracy: 84.626%\n",
      "[023/00032] train_loss: 0.488\n",
      "[023/00082] train_loss: 0.486\n",
      "[023/00082] val_loss: 0.495, val_accuracy: 84.672%\n",
      "[023/00132] train_loss: 0.529\n",
      "[023/00182] train_loss: 0.487\n",
      "[023/00182] val_loss: 0.481, val_accuracy: 84.580%\n",
      "[023/00232] train_loss: 0.490\n",
      "[023/00282] train_loss: 0.463\n",
      "[023/00282] val_loss: 0.492, val_accuracy: 83.917%\n",
      "[023/00332] train_loss: 0.487\n",
      "[023/00382] train_loss: 0.489\n",
      "[023/00382] val_loss: 0.488, val_accuracy: 84.397%\n",
      "[023/00432] train_loss: 0.520\n",
      "[024/00003] train_loss: 0.487\n",
      "[024/00003] val_loss: 0.488, val_accuracy: 84.489%\n",
      "[024/00053] train_loss: 0.499\n",
      "[024/00103] train_loss: 0.516\n",
      "[024/00103] val_loss: 0.479, val_accuracy: 84.420%\n",
      "[024/00153] train_loss: 0.493\n",
      "[024/00203] train_loss: 0.528\n",
      "[024/00203] val_loss: 0.491, val_accuracy: 84.420%\n",
      "[024/00253] train_loss: 0.467\n",
      "[024/00303] train_loss: 0.521\n",
      "[024/00303] val_loss: 0.489, val_accuracy: 84.420%\n",
      "[024/00353] train_loss: 0.472\n",
      "[024/00403] train_loss: 0.489\n",
      "[024/00403] val_loss: 0.494, val_accuracy: 83.917%\n",
      "[024/00453] train_loss: 0.488\n",
      "[025/00024] train_loss: 0.490\n",
      "[025/00024] val_loss: 0.481, val_accuracy: 84.443%\n",
      "[025/00074] train_loss: 0.499\n",
      "[025/00124] train_loss: 0.513\n",
      "[025/00124] val_loss: 0.487, val_accuracy: 84.329%\n",
      "[025/00174] train_loss: 0.488\n",
      "[025/00224] train_loss: 0.489\n",
      "[025/00224] val_loss: 0.489, val_accuracy: 84.123%\n",
      "[025/00274] train_loss: 0.476\n",
      "[025/00324] train_loss: 0.508\n",
      "[025/00324] val_loss: 0.489, val_accuracy: 84.100%\n",
      "[025/00374] train_loss: 0.484\n",
      "[025/00424] train_loss: 0.481\n",
      "[025/00424] val_loss: 0.485, val_accuracy: 84.397%\n",
      "[025/00474] train_loss: 0.492\n",
      "[026/00045] train_loss: 0.485\n",
      "[026/00045] val_loss: 0.512, val_accuracy: 83.345%\n",
      "[026/00095] train_loss: 0.502\n",
      "[026/00145] train_loss: 0.512\n",
      "[026/00145] val_loss: 0.482, val_accuracy: 84.626%\n",
      "[026/00195] train_loss: 0.484\n",
      "[026/00245] train_loss: 0.482\n",
      "[026/00245] val_loss: 0.475, val_accuracy: 84.695%\n",
      "[026/00295] train_loss: 0.497\n",
      "[026/00345] train_loss: 0.497\n",
      "[026/00345] val_loss: 0.481, val_accuracy: 84.992%\n",
      "[026/00395] train_loss: 0.480\n",
      "[026/00445] train_loss: 0.479\n",
      "[026/00445] val_loss: 0.495, val_accuracy: 84.214%\n",
      "[027/00016] train_loss: 0.469\n",
      "[027/00066] train_loss: 0.484\n",
      "[027/00066] val_loss: 0.482, val_accuracy: 84.168%\n",
      "[027/00116] train_loss: 0.477\n",
      "[027/00166] train_loss: 0.517\n",
      "[027/00166] val_loss: 0.483, val_accuracy: 84.763%\n",
      "[027/00216] train_loss: 0.509\n",
      "[027/00266] train_loss: 0.516\n",
      "[027/00266] val_loss: 0.481, val_accuracy: 84.649%\n",
      "[027/00316] train_loss: 0.467\n",
      "[027/00366] train_loss: 0.467\n",
      "[027/00366] val_loss: 0.495, val_accuracy: 83.734%\n",
      "[027/00416] train_loss: 0.508\n",
      "[027/00466] train_loss: 0.498\n",
      "[027/00466] val_loss: 0.493, val_accuracy: 84.374%\n",
      "[028/00037] train_loss: 0.502\n",
      "[028/00087] train_loss: 0.489\n",
      "[028/00087] val_loss: 0.476, val_accuracy: 84.717%\n",
      "[028/00137] train_loss: 0.517\n",
      "[028/00187] train_loss: 0.483\n",
      "[028/00187] val_loss: 0.498, val_accuracy: 84.008%\n",
      "[028/00237] train_loss: 0.505\n",
      "[028/00287] train_loss: 0.464\n",
      "[028/00287] val_loss: 0.475, val_accuracy: 84.443%\n",
      "[028/00337] train_loss: 0.476\n",
      "[028/00387] train_loss: 0.493\n",
      "[028/00387] val_loss: 0.490, val_accuracy: 84.283%\n",
      "[028/00437] train_loss: 0.482\n",
      "[029/00008] train_loss: 0.464\n",
      "[029/00008] val_loss: 0.476, val_accuracy: 85.061%\n",
      "[029/00058] train_loss: 0.478\n",
      "[029/00108] train_loss: 0.511\n",
      "[029/00108] val_loss: 0.481, val_accuracy: 84.786%\n",
      "[029/00158] train_loss: 0.499\n",
      "[029/00208] train_loss: 0.507\n",
      "[029/00208] val_loss: 0.482, val_accuracy: 84.603%\n",
      "[029/00258] train_loss: 0.485\n",
      "[029/00308] train_loss: 0.492\n",
      "[029/00308] val_loss: 0.491, val_accuracy: 84.054%\n",
      "[029/00358] train_loss: 0.487\n",
      "[029/00408] train_loss: 0.476\n",
      "[029/00408] val_loss: 0.497, val_accuracy: 84.306%\n",
      "[029/00458] train_loss: 0.465\n"
     ]
    }
   ],
   "source": [
    "from mvcnn_rec.training import train_mvcnn\n",
    "train_mvcnn.main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de932813",
   "metadata": {},
   "source": [
    "#### Finetune results epoch 30\n",
    "- [029/00408] val_loss: 0.497, val_accuracy: 84.306%\n",
    "- [029/00458] train_loss: 0.465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba690aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of mvcnn_rec.training.train_mvcnn failed: Traceback (most recent call last):\n",
      "  File \"/rhome/cuonghn/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/rhome/cuonghn/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/cluster/51/cuonghn/MVCNN_Reconstruction/mvcnn_rec/training/train_mvcnn.py\", line 7, in <module>\n",
      "    from datatime import datetime\n",
      "ModuleNotFoundError: No module named 'datatime'\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 0.495\n",
      "[000/00099] train_loss: 0.468\n",
      "[000/00099] val_loss: 0.488, val_accuracy: 84.306%\n",
      "[000/00149] train_loss: 0.469\n",
      "[000/00199] train_loss: 0.481\n",
      "[000/00199] val_loss: 0.493, val_accuracy: 83.848%\n",
      "[000/00249] train_loss: 0.483\n",
      "[000/00299] train_loss: 0.489\n",
      "[000/00299] val_loss: 0.490, val_accuracy: 84.374%\n",
      "[000/00349] train_loss: 0.499\n",
      "[000/00399] train_loss: 0.504\n",
      "[000/00399] val_loss: 0.491, val_accuracy: 84.306%\n",
      "[000/00449] train_loss: 0.489\n",
      "[001/00020] train_loss: 0.488\n",
      "[001/00020] val_loss: 0.483, val_accuracy: 85.015%\n",
      "[001/00070] train_loss: 0.475\n",
      "[001/00120] train_loss: 0.478\n",
      "[001/00120] val_loss: 0.473, val_accuracy: 84.626%\n",
      "[001/00170] train_loss: 0.509\n",
      "[001/00220] train_loss: 0.465\n",
      "[001/00220] val_loss: 0.494, val_accuracy: 83.894%\n",
      "[001/00270] train_loss: 0.528\n",
      "[001/00320] train_loss: 0.467\n",
      "[001/00320] val_loss: 0.481, val_accuracy: 84.397%\n",
      "[001/00370] train_loss: 0.508\n",
      "[001/00420] train_loss: 0.486\n",
      "[001/00420] val_loss: 0.478, val_accuracy: 84.443%\n",
      "[001/00470] train_loss: 0.508\n",
      "[002/00041] train_loss: 0.492\n",
      "[002/00041] val_loss: 0.487, val_accuracy: 84.283%\n",
      "[002/00091] train_loss: 0.489\n",
      "[002/00141] train_loss: 0.479\n",
      "[002/00141] val_loss: 0.486, val_accuracy: 84.329%\n",
      "[002/00191] train_loss: 0.486\n",
      "[002/00241] train_loss: 0.504\n",
      "[002/00241] val_loss: 0.486, val_accuracy: 84.443%\n",
      "[002/00291] train_loss: 0.518\n",
      "[002/00341] train_loss: 0.476\n",
      "[002/00341] val_loss: 0.485, val_accuracy: 84.534%\n",
      "[002/00391] train_loss: 0.487\n",
      "[002/00441] train_loss: 0.506\n",
      "[002/00441] val_loss: 0.490, val_accuracy: 84.100%\n",
      "[003/00012] train_loss: 0.498\n",
      "[003/00062] train_loss: 0.480\n",
      "[003/00062] val_loss: 0.480, val_accuracy: 84.443%\n",
      "[003/00112] train_loss: 0.529\n",
      "[003/00162] train_loss: 0.478\n",
      "[003/00162] val_loss: 0.480, val_accuracy: 84.580%\n",
      "[003/00212] train_loss: 0.491\n",
      "[003/00262] train_loss: 0.492\n",
      "[003/00262] val_loss: 0.487, val_accuracy: 84.260%\n",
      "[003/00312] train_loss: 0.476\n",
      "[003/00362] train_loss: 0.488\n",
      "[003/00362] val_loss: 0.491, val_accuracy: 84.306%\n",
      "[003/00412] train_loss: 0.472\n",
      "[003/00462] train_loss: 0.498\n",
      "[003/00462] val_loss: 0.495, val_accuracy: 84.306%\n",
      "[004/00033] train_loss: 0.493\n",
      "[004/00083] train_loss: 0.487\n",
      "[004/00083] val_loss: 0.486, val_accuracy: 84.283%\n",
      "[004/00133] train_loss: 0.501\n",
      "[004/00183] train_loss: 0.489\n",
      "[004/00183] val_loss: 0.488, val_accuracy: 84.283%\n",
      "[004/00233] train_loss: 0.470\n",
      "[004/00283] train_loss: 0.463\n",
      "[004/00283] val_loss: 0.481, val_accuracy: 84.351%\n",
      "[004/00333] train_loss: 0.468\n",
      "[004/00383] train_loss: 0.473\n",
      "[004/00383] val_loss: 0.487, val_accuracy: 83.940%\n",
      "[004/00433] train_loss: 0.505\n",
      "[005/00004] train_loss: 0.486\n",
      "[005/00004] val_loss: 0.500, val_accuracy: 83.619%\n",
      "[005/00054] train_loss: 0.487\n",
      "[005/00104] train_loss: 0.463\n",
      "[005/00104] val_loss: 0.490, val_accuracy: 84.329%\n",
      "[005/00154] train_loss: 0.455\n",
      "[005/00204] train_loss: 0.460\n",
      "[005/00204] val_loss: 0.490, val_accuracy: 84.008%\n",
      "[005/00254] train_loss: 0.483\n",
      "[005/00304] train_loss: 0.508\n",
      "[005/00304] val_loss: 0.483, val_accuracy: 84.626%\n",
      "[005/00354] train_loss: 0.499\n",
      "[005/00404] train_loss: 0.492\n",
      "[005/00404] val_loss: 0.486, val_accuracy: 84.351%\n",
      "[005/00454] train_loss: 0.527\n",
      "[006/00025] train_loss: 0.513\n",
      "[006/00025] val_loss: 0.493, val_accuracy: 84.351%\n",
      "[006/00075] train_loss: 0.482\n",
      "[006/00125] train_loss: 0.481\n",
      "[006/00125] val_loss: 0.501, val_accuracy: 83.505%\n",
      "[006/00175] train_loss: 0.490\n",
      "[006/00225] train_loss: 0.490\n",
      "[006/00225] val_loss: 0.479, val_accuracy: 84.168%\n",
      "[006/00275] train_loss: 0.487\n",
      "[006/00325] train_loss: 0.509\n",
      "[006/00325] val_loss: 0.482, val_accuracy: 84.672%\n",
      "[006/00375] train_loss: 0.466\n",
      "[006/00425] train_loss: 0.470\n",
      "[006/00425] val_loss: 0.491, val_accuracy: 84.534%\n",
      "[006/00475] train_loss: 0.479\n",
      "[007/00046] train_loss: 0.486\n",
      "[007/00046] val_loss: 0.478, val_accuracy: 85.198%\n",
      "[007/00096] train_loss: 0.476\n",
      "[007/00146] train_loss: 0.502\n",
      "[007/00146] val_loss: 0.483, val_accuracy: 84.649%\n",
      "[007/00196] train_loss: 0.484\n",
      "[007/00246] train_loss: 0.483\n",
      "[007/00246] val_loss: 0.477, val_accuracy: 84.740%\n",
      "[007/00296] train_loss: 0.500\n",
      "[007/00346] train_loss: 0.484\n",
      "[007/00346] val_loss: 0.476, val_accuracy: 84.534%\n",
      "[007/00396] train_loss: 0.477\n",
      "[007/00446] train_loss: 0.502\n",
      "[007/00446] val_loss: 0.514, val_accuracy: 83.528%\n",
      "[008/00017] train_loss: 0.491\n",
      "[008/00067] train_loss: 0.482\n",
      "[008/00067] val_loss: 0.476, val_accuracy: 84.626%\n",
      "[008/00117] train_loss: 0.462\n",
      "[008/00167] train_loss: 0.480\n",
      "[008/00167] val_loss: 0.482, val_accuracy: 84.580%\n",
      "[008/00217] train_loss: 0.489\n",
      "[008/00267] train_loss: 0.515\n",
      "[008/00267] val_loss: 0.490, val_accuracy: 84.329%\n",
      "[008/00317] train_loss: 0.514\n",
      "[008/00367] train_loss: 0.469\n",
      "[008/00367] val_loss: 0.487, val_accuracy: 84.512%\n",
      "[008/00417] train_loss: 0.496\n",
      "[008/00467] train_loss: 0.462\n",
      "[008/00467] val_loss: 0.482, val_accuracy: 84.534%\n",
      "[009/00038] train_loss: 0.501\n",
      "[009/00088] train_loss: 0.480\n",
      "[009/00088] val_loss: 0.487, val_accuracy: 84.168%\n",
      "[009/00138] train_loss: 0.502\n",
      "[009/00188] train_loss: 0.485\n",
      "[009/00188] val_loss: 0.481, val_accuracy: 84.717%\n",
      "[009/00238] train_loss: 0.489\n",
      "[009/00288] train_loss: 0.526\n",
      "[009/00288] val_loss: 0.490, val_accuracy: 84.214%\n",
      "[009/00338] train_loss: 0.458\n",
      "[009/00388] train_loss: 0.504\n",
      "[009/00388] val_loss: 0.495, val_accuracy: 83.917%\n",
      "[009/00438] train_loss: 0.496\n",
      "[010/00009] train_loss: 0.510\n",
      "[010/00009] val_loss: 0.511, val_accuracy: 83.940%\n",
      "[010/00059] train_loss: 0.490\n",
      "[010/00109] train_loss: 0.486\n",
      "[010/00109] val_loss: 0.491, val_accuracy: 84.397%\n",
      "[010/00159] train_loss: 0.490\n",
      "[010/00209] train_loss: 0.490\n",
      "[010/00209] val_loss: 0.484, val_accuracy: 83.871%\n",
      "[010/00259] train_loss: 0.495\n",
      "[010/00309] train_loss: 0.462\n",
      "[010/00309] val_loss: 0.478, val_accuracy: 84.649%\n",
      "[010/00359] train_loss: 0.504\n",
      "[010/00409] train_loss: 0.496\n",
      "[010/00409] val_loss: 0.482, val_accuracy: 84.306%\n",
      "[010/00459] train_loss: 0.486\n",
      "[011/00030] train_loss: 0.490\n",
      "[011/00030] val_loss: 0.485, val_accuracy: 84.489%\n",
      "[011/00080] train_loss: 0.522\n",
      "[011/00130] train_loss: 0.458\n",
      "[011/00130] val_loss: 0.499, val_accuracy: 83.962%\n",
      "[011/00180] train_loss: 0.502\n",
      "[011/00230] train_loss: 0.483\n",
      "[011/00230] val_loss: 0.501, val_accuracy: 84.008%\n",
      "[011/00280] train_loss: 0.471\n",
      "[011/00330] train_loss: 0.517\n",
      "[011/00330] val_loss: 0.485, val_accuracy: 84.351%\n",
      "[011/00380] train_loss: 0.496\n",
      "[011/00430] train_loss: 0.448\n",
      "[011/00430] val_loss: 0.486, val_accuracy: 84.420%\n",
      "[012/00001] train_loss: 0.511\n",
      "[012/00051] train_loss: 0.476\n",
      "[012/00051] val_loss: 0.484, val_accuracy: 84.878%\n",
      "[012/00101] train_loss: 0.468\n",
      "[012/00151] train_loss: 0.508\n",
      "[012/00151] val_loss: 0.480, val_accuracy: 84.031%\n",
      "[012/00201] train_loss: 0.461\n",
      "[012/00251] train_loss: 0.485\n",
      "[012/00251] val_loss: 0.487, val_accuracy: 84.512%\n",
      "[012/00301] train_loss: 0.497\n",
      "[012/00351] train_loss: 0.488\n",
      "[012/00351] val_loss: 0.489, val_accuracy: 84.557%\n",
      "[012/00401] train_loss: 0.467\n",
      "[012/00451] train_loss: 0.482\n",
      "[012/00451] val_loss: 0.476, val_accuracy: 84.946%\n",
      "[013/00022] train_loss: 0.522\n",
      "[013/00072] train_loss: 0.491\n",
      "[013/00072] val_loss: 0.493, val_accuracy: 84.466%\n",
      "[013/00122] train_loss: 0.481\n",
      "[013/00172] train_loss: 0.490\n",
      "[013/00172] val_loss: 0.489, val_accuracy: 84.489%\n",
      "[013/00222] train_loss: 0.462\n",
      "[013/00272] train_loss: 0.492\n",
      "[013/00272] val_loss: 0.481, val_accuracy: 84.900%\n",
      "[013/00322] train_loss: 0.508\n",
      "[013/00372] train_loss: 0.474\n",
      "[013/00372] val_loss: 0.489, val_accuracy: 84.580%\n",
      "[013/00422] train_loss: 0.483\n",
      "[013/00472] train_loss: 0.505\n",
      "[013/00472] val_loss: 0.489, val_accuracy: 84.077%\n",
      "[014/00043] train_loss: 0.482\n",
      "[014/00093] train_loss: 0.498\n",
      "[014/00093] val_loss: 0.486, val_accuracy: 84.534%\n",
      "[014/00143] train_loss: 0.462\n",
      "[014/00193] train_loss: 0.505\n",
      "[014/00193] val_loss: 0.486, val_accuracy: 84.603%\n",
      "[014/00243] train_loss: 0.483\n",
      "[014/00293] train_loss: 0.497\n",
      "[014/00293] val_loss: 0.491, val_accuracy: 84.283%\n",
      "[014/00343] train_loss: 0.483\n",
      "[014/00393] train_loss: 0.497\n",
      "[014/00393] val_loss: 0.475, val_accuracy: 84.763%\n",
      "[014/00443] train_loss: 0.472\n",
      "[015/00014] train_loss: 0.482\n",
      "[015/00014] val_loss: 0.489, val_accuracy: 84.168%\n",
      "[015/00064] train_loss: 0.497\n",
      "[015/00114] train_loss: 0.457\n",
      "[015/00114] val_loss: 0.482, val_accuracy: 84.512%\n",
      "[015/00164] train_loss: 0.453\n",
      "[015/00214] train_loss: 0.471\n",
      "[015/00214] val_loss: 0.487, val_accuracy: 84.466%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[015/00264] train_loss: 0.501\n",
      "[015/00314] train_loss: 0.497\n",
      "[015/00314] val_loss: 0.499, val_accuracy: 84.054%\n",
      "[015/00364] train_loss: 0.527\n",
      "[015/00414] train_loss: 0.494\n",
      "[015/00414] val_loss: 0.489, val_accuracy: 84.306%\n",
      "[015/00464] train_loss: 0.506\n",
      "[016/00035] train_loss: 0.478\n",
      "[016/00035] val_loss: 0.490, val_accuracy: 84.329%\n",
      "[016/00085] train_loss: 0.500\n",
      "[016/00135] train_loss: 0.492\n",
      "[016/00135] val_loss: 0.490, val_accuracy: 84.260%\n",
      "[016/00185] train_loss: 0.496\n",
      "[016/00235] train_loss: 0.470\n",
      "[016/00235] val_loss: 0.485, val_accuracy: 84.534%\n",
      "[016/00285] train_loss: 0.499\n",
      "[016/00335] train_loss: 0.468\n",
      "[016/00335] val_loss: 0.496, val_accuracy: 83.757%\n",
      "[016/00385] train_loss: 0.511\n",
      "[016/00435] train_loss: 0.498\n",
      "[016/00435] val_loss: 0.476, val_accuracy: 85.198%\n",
      "[017/00006] train_loss: 0.471\n",
      "[017/00056] train_loss: 0.512\n",
      "[017/00056] val_loss: 0.491, val_accuracy: 83.894%\n",
      "[017/00106] train_loss: 0.469\n",
      "[017/00156] train_loss: 0.470\n",
      "[017/00156] val_loss: 0.480, val_accuracy: 84.329%\n",
      "[017/00206] train_loss: 0.497\n",
      "[017/00256] train_loss: 0.497\n",
      "[017/00256] val_loss: 0.495, val_accuracy: 84.283%\n",
      "[017/00306] train_loss: 0.475\n",
      "[017/00356] train_loss: 0.471\n",
      "[017/00356] val_loss: 0.492, val_accuracy: 83.985%\n",
      "[017/00406] train_loss: 0.506\n",
      "[017/00456] train_loss: 0.491\n",
      "[017/00456] val_loss: 0.480, val_accuracy: 85.106%\n",
      "[018/00027] train_loss: 0.492\n",
      "[018/00077] train_loss: 0.472\n",
      "[018/00077] val_loss: 0.487, val_accuracy: 84.534%\n",
      "[018/00127] train_loss: 0.493\n",
      "[018/00177] train_loss: 0.473\n",
      "[018/00177] val_loss: 0.494, val_accuracy: 84.351%\n",
      "[018/00227] train_loss: 0.490\n",
      "[018/00277] train_loss: 0.488\n",
      "[018/00277] val_loss: 0.481, val_accuracy: 84.397%\n",
      "[018/00327] train_loss: 0.508\n",
      "[018/00377] train_loss: 0.484\n",
      "[018/00377] val_loss: 0.475, val_accuracy: 84.763%\n",
      "[018/00427] train_loss: 0.500\n",
      "[018/00477] train_loss: 0.494\n",
      "[018/00477] val_loss: 0.489, val_accuracy: 84.626%\n",
      "[019/00048] train_loss: 0.448\n",
      "[019/00098] train_loss: 0.520\n",
      "[019/00098] val_loss: 0.484, val_accuracy: 84.695%\n",
      "[019/00148] train_loss: 0.486\n",
      "[019/00198] train_loss: 0.484\n",
      "[019/00198] val_loss: 0.496, val_accuracy: 84.260%\n",
      "[019/00248] train_loss: 0.480\n",
      "[019/00298] train_loss: 0.505\n",
      "[019/00298] val_loss: 0.477, val_accuracy: 84.512%\n",
      "[019/00348] train_loss: 0.461\n",
      "[019/00398] train_loss: 0.499\n",
      "[019/00398] val_loss: 0.493, val_accuracy: 84.351%\n",
      "[019/00448] train_loss: 0.484\n",
      "[020/00019] train_loss: 0.498\n",
      "[020/00019] val_loss: 0.484, val_accuracy: 84.580%\n",
      "[020/00069] train_loss: 0.497\n",
      "[020/00119] train_loss: 0.478\n",
      "[020/00119] val_loss: 0.490, val_accuracy: 84.466%\n",
      "[020/00169] train_loss: 0.505\n",
      "[020/00219] train_loss: 0.459\n",
      "[020/00219] val_loss: 0.482, val_accuracy: 84.969%\n",
      "[020/00269] train_loss: 0.499\n",
      "[020/00319] train_loss: 0.473\n",
      "[020/00319] val_loss: 0.482, val_accuracy: 84.534%\n",
      "[020/00369] train_loss: 0.498\n",
      "[020/00419] train_loss: 0.466\n",
      "[020/00419] val_loss: 0.488, val_accuracy: 84.580%\n",
      "[020/00469] train_loss: 0.522\n",
      "[021/00040] train_loss: 0.498\n",
      "[021/00040] val_loss: 0.477, val_accuracy: 84.740%\n",
      "[021/00090] train_loss: 0.475\n",
      "[021/00140] train_loss: 0.503\n",
      "[021/00140] val_loss: 0.488, val_accuracy: 84.397%\n",
      "[021/00190] train_loss: 0.494\n",
      "[021/00240] train_loss: 0.497\n",
      "[021/00240] val_loss: 0.486, val_accuracy: 84.443%\n",
      "[021/00290] train_loss: 0.468\n",
      "[021/00340] train_loss: 0.508\n",
      "[021/00340] val_loss: 0.477, val_accuracy: 84.695%\n",
      "[021/00390] train_loss: 0.481\n",
      "[021/00440] train_loss: 0.476\n",
      "[021/00440] val_loss: 0.485, val_accuracy: 84.672%\n",
      "[022/00011] train_loss: 0.481\n",
      "[022/00061] train_loss: 0.478\n",
      "[022/00061] val_loss: 0.483, val_accuracy: 84.992%\n",
      "[022/00111] train_loss: 0.475\n",
      "[022/00161] train_loss: 0.521\n",
      "[022/00161] val_loss: 0.495, val_accuracy: 84.168%\n",
      "[022/00211] train_loss: 0.475\n",
      "[022/00261] train_loss: 0.521\n",
      "[022/00261] val_loss: 0.484, val_accuracy: 84.260%\n",
      "[022/00311] train_loss: 0.478\n",
      "[022/00361] train_loss: 0.471\n",
      "[022/00361] val_loss: 0.491, val_accuracy: 84.420%\n",
      "[022/00411] train_loss: 0.466\n",
      "[022/00461] train_loss: 0.461\n",
      "[022/00461] val_loss: 0.494, val_accuracy: 84.672%\n",
      "[023/00032] train_loss: 0.506\n",
      "[023/00082] train_loss: 0.469\n",
      "[023/00082] val_loss: 0.497, val_accuracy: 84.054%\n",
      "[023/00132] train_loss: 0.462\n",
      "[023/00182] train_loss: 0.512\n",
      "[023/00182] val_loss: 0.481, val_accuracy: 84.512%\n",
      "[023/00232] train_loss: 0.467\n",
      "[023/00282] train_loss: 0.484\n",
      "[023/00282] val_loss: 0.480, val_accuracy: 84.992%\n",
      "[023/00332] train_loss: 0.519\n",
      "[023/00382] train_loss: 0.465\n",
      "[023/00382] val_loss: 0.491, val_accuracy: 84.626%\n",
      "[023/00432] train_loss: 0.482\n",
      "[024/00003] train_loss: 0.481\n",
      "[024/00003] val_loss: 0.476, val_accuracy: 84.717%\n",
      "[024/00053] train_loss: 0.473\n",
      "[024/00103] train_loss: 0.483\n",
      "[024/00103] val_loss: 0.501, val_accuracy: 83.596%\n",
      "[024/00153] train_loss: 0.496\n",
      "[024/00203] train_loss: 0.494\n",
      "[024/00203] val_loss: 0.480, val_accuracy: 85.129%\n",
      "[024/00253] train_loss: 0.472\n",
      "[024/00303] train_loss: 0.486\n",
      "[024/00303] val_loss: 0.488, val_accuracy: 84.786%\n",
      "[024/00353] train_loss: 0.503\n",
      "[024/00403] train_loss: 0.475\n",
      "[024/00403] val_loss: 0.485, val_accuracy: 84.466%\n",
      "[024/00453] train_loss: 0.499\n",
      "[025/00024] train_loss: 0.519\n",
      "[025/00024] val_loss: 0.477, val_accuracy: 84.809%\n",
      "[025/00074] train_loss: 0.487\n",
      "[025/00124] train_loss: 0.491\n",
      "[025/00124] val_loss: 0.478, val_accuracy: 84.397%\n",
      "[025/00174] train_loss: 0.477\n",
      "[025/00224] train_loss: 0.464\n",
      "[025/00224] val_loss: 0.477, val_accuracy: 84.786%\n",
      "[025/00274] train_loss: 0.493\n",
      "[025/00324] train_loss: 0.510\n",
      "[025/00324] val_loss: 0.472, val_accuracy: 84.992%\n",
      "[025/00374] train_loss: 0.494\n",
      "[025/00424] train_loss: 0.489\n",
      "[025/00424] val_loss: 0.476, val_accuracy: 84.534%\n",
      "[025/00474] train_loss: 0.488\n",
      "[026/00045] train_loss: 0.474\n",
      "[026/00045] val_loss: 0.482, val_accuracy: 84.557%\n",
      "[026/00095] train_loss: 0.456\n",
      "[026/00145] train_loss: 0.472\n",
      "[026/00145] val_loss: 0.492, val_accuracy: 84.146%\n",
      "[026/00195] train_loss: 0.470\n",
      "[026/00245] train_loss: 0.487\n",
      "[026/00245] val_loss: 0.496, val_accuracy: 84.168%\n",
      "[026/00295] train_loss: 0.491\n",
      "[026/00345] train_loss: 0.521\n",
      "[026/00345] val_loss: 0.496, val_accuracy: 84.420%\n",
      "[026/00395] train_loss: 0.471\n",
      "[026/00445] train_loss: 0.489\n",
      "[026/00445] val_loss: 0.488, val_accuracy: 84.397%\n",
      "[027/00016] train_loss: 0.476\n",
      "[027/00066] train_loss: 0.492\n",
      "[027/00066] val_loss: 0.494, val_accuracy: 84.283%\n",
      "[027/00116] train_loss: 0.468\n",
      "[027/00166] train_loss: 0.505\n",
      "[027/00166] val_loss: 0.492, val_accuracy: 84.420%\n",
      "[027/00216] train_loss: 0.460\n",
      "[027/00266] train_loss: 0.477\n",
      "[027/00266] val_loss: 0.489, val_accuracy: 84.512%\n",
      "[027/00316] train_loss: 0.464\n",
      "[027/00366] train_loss: 0.482\n",
      "[027/00366] val_loss: 0.473, val_accuracy: 84.763%\n",
      "[027/00416] train_loss: 0.492\n",
      "[027/00466] train_loss: 0.505\n",
      "[027/00466] val_loss: 0.481, val_accuracy: 84.672%\n",
      "[028/00037] train_loss: 0.504\n",
      "[028/00087] train_loss: 0.499\n",
      "[028/00087] val_loss: 0.488, val_accuracy: 84.397%\n",
      "[028/00137] train_loss: 0.467\n",
      "[028/00187] train_loss: 0.495\n",
      "[028/00187] val_loss: 0.488, val_accuracy: 84.557%\n",
      "[028/00237] train_loss: 0.461\n",
      "[028/00287] train_loss: 0.470\n",
      "[028/00287] val_loss: 0.481, val_accuracy: 84.603%\n",
      "[028/00337] train_loss: 0.500\n",
      "[028/00387] train_loss: 0.485\n",
      "[028/00387] val_loss: 0.483, val_accuracy: 84.420%\n",
      "[028/00437] train_loss: 0.491\n",
      "[029/00008] train_loss: 0.490\n",
      "[029/00008] val_loss: 0.483, val_accuracy: 84.763%\n",
      "[029/00058] train_loss: 0.490\n",
      "[029/00108] train_loss: 0.482\n",
      "[029/00108] val_loss: 0.484, val_accuracy: 84.123%\n",
      "[029/00158] train_loss: 0.459\n",
      "[029/00208] train_loss: 0.463\n",
      "[029/00208] val_loss: 0.485, val_accuracy: 84.283%\n",
      "[029/00258] train_loss: 0.477\n",
      "[029/00308] train_loss: 0.495\n",
      "[029/00308] val_loss: 0.489, val_accuracy: 84.580%\n",
      "[029/00358] train_loss: 0.496\n",
      "[029/00408] train_loss: 0.497\n",
      "[029/00408] val_loss: 0.483, val_accuracy: 84.534%\n",
      "[029/00458] train_loss: 0.486\n"
     ]
    }
   ],
   "source": [
    "# More 30 epochs\n",
    "from mvcnn_rec.training import train_mvcnn\n",
    "config = {\n",
    "    'experiment_name': '1vcnn_generalize',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'resume_ckpt': 'mvcnn_rec/runs/1vcnn_generalize/model_best.ckpt',\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 30,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 100,\n",
    "    'num_views': 1,\n",
    "    'random_start_view': True, \n",
    "    'freezee_backbone': True\n",
    "}\n",
    "train_mvcnn.main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d61a5a",
   "metadata": {},
   "source": [
    "#### Performance do not improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c07dbe8",
   "metadata": {},
   "source": [
    "#### Finetune backbone resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e559b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of mvcnn_rec.training.train_mvcnn failed: Traceback (most recent call last):\n",
      "  File \"/rhome/cuonghn/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/rhome/cuonghn/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/cluster/51/cuonghn/MVCNN_Reconstruction/mvcnn_rec/training/train_mvcnn.py\", line 7, in <module>\n",
      "    from datatime import datetime\n",
      "ModuleNotFoundError: No module named 'datatime'\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000/00049] train_loss: 0.475\n",
      "[000/00099] train_loss: 0.446\n",
      "[000/00099] val_loss: 0.470, val_accuracy: 85.427%\n",
      "[000/00149] train_loss: 0.490\n",
      "[000/00199] train_loss: 0.448\n",
      "[000/00199] val_loss: 0.469, val_accuracy: 84.992%\n",
      "[000/00249] train_loss: 0.443\n",
      "[000/00299] train_loss: 0.458\n",
      "[000/00299] val_loss: 0.475, val_accuracy: 84.809%\n",
      "[000/00349] train_loss: 0.477\n",
      "[000/00399] train_loss: 0.452\n",
      "[000/00399] val_loss: 0.470, val_accuracy: 85.152%\n",
      "[000/00449] train_loss: 0.483\n",
      "[001/00020] train_loss: 0.472\n",
      "[001/00020] val_loss: 0.467, val_accuracy: 85.289%\n",
      "[001/00070] train_loss: 0.498\n",
      "[001/00120] train_loss: 0.446\n",
      "[001/00120] val_loss: 0.474, val_accuracy: 84.878%\n",
      "[001/00170] train_loss: 0.468\n",
      "[001/00220] train_loss: 0.514\n",
      "[001/00220] val_loss: 0.468, val_accuracy: 85.106%\n",
      "[001/00270] train_loss: 0.468\n",
      "[001/00320] train_loss: 0.456\n",
      "[001/00320] val_loss: 0.473, val_accuracy: 84.900%\n",
      "[001/00370] train_loss: 0.451\n",
      "[001/00420] train_loss: 0.469\n",
      "[001/00420] val_loss: 0.467, val_accuracy: 85.289%\n",
      "[001/00470] train_loss: 0.489\n",
      "[002/00041] train_loss: 0.448\n",
      "[002/00041] val_loss: 0.463, val_accuracy: 85.289%\n",
      "[002/00091] train_loss: 0.458\n",
      "[002/00141] train_loss: 0.436\n",
      "[002/00141] val_loss: 0.471, val_accuracy: 84.740%\n",
      "[002/00191] train_loss: 0.470\n",
      "[002/00241] train_loss: 0.451\n",
      "[002/00241] val_loss: 0.467, val_accuracy: 85.472%\n",
      "[002/00291] train_loss: 0.484\n",
      "[002/00341] train_loss: 0.443\n",
      "[002/00341] val_loss: 0.468, val_accuracy: 85.289%\n",
      "[002/00391] train_loss: 0.465\n",
      "[002/00441] train_loss: 0.494\n",
      "[002/00441] val_loss: 0.470, val_accuracy: 84.923%\n",
      "[003/00012] train_loss: 0.482\n",
      "[003/00062] train_loss: 0.467\n",
      "[003/00062] val_loss: 0.464, val_accuracy: 85.267%\n",
      "[003/00112] train_loss: 0.459\n",
      "[003/00162] train_loss: 0.447\n",
      "[003/00162] val_loss: 0.465, val_accuracy: 85.267%\n",
      "[003/00212] train_loss: 0.484\n",
      "[003/00262] train_loss: 0.475\n",
      "[003/00262] val_loss: 0.468, val_accuracy: 85.175%\n",
      "[003/00312] train_loss: 0.465\n",
      "[003/00362] train_loss: 0.464\n",
      "[003/00362] val_loss: 0.467, val_accuracy: 85.198%\n",
      "[003/00412] train_loss: 0.467\n",
      "[003/00462] train_loss: 0.448\n",
      "[003/00462] val_loss: 0.470, val_accuracy: 85.175%\n",
      "[004/00033] train_loss: 0.482\n",
      "[004/00083] train_loss: 0.480\n",
      "[004/00083] val_loss: 0.471, val_accuracy: 84.992%\n",
      "[004/00133] train_loss: 0.438\n",
      "[004/00183] train_loss: 0.479\n",
      "[004/00183] val_loss: 0.467, val_accuracy: 85.450%\n",
      "[004/00233] train_loss: 0.443\n",
      "[004/00283] train_loss: 0.464\n",
      "[004/00283] val_loss: 0.465, val_accuracy: 85.015%\n",
      "[004/00333] train_loss: 0.469\n",
      "[004/00383] train_loss: 0.462\n",
      "[004/00383] val_loss: 0.468, val_accuracy: 84.695%\n",
      "[004/00433] train_loss: 0.494\n",
      "[005/00004] train_loss: 0.440\n",
      "[005/00004] val_loss: 0.462, val_accuracy: 85.267%\n",
      "[005/00054] train_loss: 0.460\n",
      "[005/00104] train_loss: 0.471\n",
      "[005/00104] val_loss: 0.470, val_accuracy: 85.038%\n",
      "[005/00154] train_loss: 0.501\n",
      "[005/00204] train_loss: 0.495\n",
      "[005/00204] val_loss: 0.462, val_accuracy: 85.335%\n",
      "[005/00254] train_loss: 0.459\n",
      "[005/00304] train_loss: 0.461\n",
      "[005/00304] val_loss: 0.467, val_accuracy: 85.244%\n",
      "[005/00354] train_loss: 0.476\n",
      "[005/00404] train_loss: 0.455\n",
      "[005/00404] val_loss: 0.469, val_accuracy: 85.152%\n",
      "[005/00454] train_loss: 0.434\n",
      "[006/00025] train_loss: 0.467\n",
      "[006/00025] val_loss: 0.465, val_accuracy: 85.015%\n",
      "[006/00075] train_loss: 0.459\n",
      "[006/00125] train_loss: 0.484\n",
      "[006/00125] val_loss: 0.465, val_accuracy: 84.969%\n",
      "[006/00175] train_loss: 0.493\n",
      "[006/00225] train_loss: 0.477\n",
      "[006/00225] val_loss: 0.464, val_accuracy: 85.129%\n",
      "[006/00275] train_loss: 0.482\n",
      "[006/00325] train_loss: 0.474\n",
      "[006/00325] val_loss: 0.461, val_accuracy: 85.129%\n",
      "[006/00375] train_loss: 0.465\n",
      "[006/00425] train_loss: 0.440\n",
      "[006/00425] val_loss: 0.464, val_accuracy: 85.335%\n",
      "[006/00475] train_loss: 0.471\n",
      "[007/00046] train_loss: 0.489\n",
      "[007/00046] val_loss: 0.463, val_accuracy: 85.038%\n",
      "[007/00096] train_loss: 0.499\n",
      "[007/00146] train_loss: 0.449\n",
      "[007/00146] val_loss: 0.465, val_accuracy: 85.289%\n",
      "[007/00196] train_loss: 0.452\n",
      "[007/00246] train_loss: 0.475\n",
      "[007/00246] val_loss: 0.473, val_accuracy: 84.878%\n",
      "[007/00296] train_loss: 0.490\n",
      "[007/00346] train_loss: 0.462\n",
      "[007/00346] val_loss: 0.463, val_accuracy: 85.198%\n",
      "[007/00396] train_loss: 0.465\n",
      "[007/00446] train_loss: 0.452\n",
      "[007/00446] val_loss: 0.464, val_accuracy: 84.969%\n",
      "[008/00017] train_loss: 0.470\n",
      "[008/00067] train_loss: 0.466\n",
      "[008/00067] val_loss: 0.465, val_accuracy: 84.969%\n",
      "[008/00117] train_loss: 0.462\n",
      "[008/00167] train_loss: 0.485\n",
      "[008/00167] val_loss: 0.463, val_accuracy: 85.152%\n",
      "[008/00217] train_loss: 0.455\n",
      "[008/00267] train_loss: 0.438\n",
      "[008/00267] val_loss: 0.465, val_accuracy: 84.763%\n",
      "[008/00317] train_loss: 0.484\n",
      "[008/00367] train_loss: 0.447\n",
      "[008/00367] val_loss: 0.468, val_accuracy: 84.923%\n",
      "[008/00417] train_loss: 0.472\n",
      "[008/00467] train_loss: 0.482\n",
      "[008/00467] val_loss: 0.467, val_accuracy: 84.992%\n",
      "[009/00038] train_loss: 0.452\n",
      "[009/00088] train_loss: 0.453\n",
      "[009/00088] val_loss: 0.466, val_accuracy: 85.152%\n",
      "[009/00138] train_loss: 0.471\n",
      "[009/00188] train_loss: 0.469\n",
      "[009/00188] val_loss: 0.469, val_accuracy: 84.855%\n",
      "[009/00238] train_loss: 0.450\n",
      "[009/00288] train_loss: 0.487\n",
      "[009/00288] val_loss: 0.463, val_accuracy: 85.106%\n",
      "[009/00338] train_loss: 0.479\n",
      "[009/00388] train_loss: 0.472\n",
      "[009/00388] val_loss: 0.467, val_accuracy: 84.992%\n",
      "[009/00438] train_loss: 0.460\n",
      "[010/00009] train_loss: 0.464\n",
      "[010/00009] val_loss: 0.464, val_accuracy: 84.809%\n",
      "[010/00059] train_loss: 0.448\n",
      "[010/00109] train_loss: 0.454\n",
      "[010/00109] val_loss: 0.465, val_accuracy: 85.106%\n",
      "[010/00159] train_loss: 0.463\n",
      "[010/00209] train_loss: 0.431\n",
      "[010/00209] val_loss: 0.468, val_accuracy: 85.061%\n",
      "[010/00259] train_loss: 0.460\n",
      "[010/00309] train_loss: 0.457\n",
      "[010/00309] val_loss: 0.465, val_accuracy: 85.152%\n",
      "[010/00359] train_loss: 0.473\n",
      "[010/00409] train_loss: 0.461\n",
      "[010/00409] val_loss: 0.464, val_accuracy: 85.221%\n",
      "[010/00459] train_loss: 0.461\n",
      "[011/00030] train_loss: 0.478\n",
      "[011/00030] val_loss: 0.467, val_accuracy: 84.969%\n",
      "[011/00080] train_loss: 0.435\n",
      "[011/00130] train_loss: 0.479\n",
      "[011/00130] val_loss: 0.467, val_accuracy: 85.084%\n",
      "[011/00180] train_loss: 0.462\n",
      "[011/00230] train_loss: 0.459\n",
      "[011/00230] val_loss: 0.466, val_accuracy: 84.946%\n",
      "[011/00280] train_loss: 0.488\n",
      "[011/00330] train_loss: 0.473\n",
      "[011/00330] val_loss: 0.465, val_accuracy: 85.061%\n",
      "[011/00380] train_loss: 0.465\n",
      "[011/00430] train_loss: 0.450\n",
      "[011/00430] val_loss: 0.464, val_accuracy: 84.878%\n",
      "[012/00001] train_loss: 0.473\n",
      "[012/00051] train_loss: 0.459\n",
      "[012/00051] val_loss: 0.471, val_accuracy: 84.855%\n",
      "[012/00101] train_loss: 0.448\n",
      "[012/00151] train_loss: 0.501\n",
      "[012/00151] val_loss: 0.467, val_accuracy: 84.992%\n",
      "[012/00201] train_loss: 0.469\n",
      "[012/00251] train_loss: 0.457\n",
      "[012/00251] val_loss: 0.462, val_accuracy: 85.541%\n",
      "[012/00301] train_loss: 0.466\n",
      "[012/00351] train_loss: 0.450\n",
      "[012/00351] val_loss: 0.465, val_accuracy: 85.061%\n",
      "[012/00401] train_loss: 0.479\n",
      "[012/00451] train_loss: 0.460\n",
      "[012/00451] val_loss: 0.459, val_accuracy: 85.381%\n",
      "[013/00022] train_loss: 0.481\n",
      "[013/00072] train_loss: 0.476\n",
      "[013/00072] val_loss: 0.464, val_accuracy: 85.221%\n",
      "[013/00122] train_loss: 0.467\n",
      "[013/00172] train_loss: 0.466\n",
      "[013/00172] val_loss: 0.465, val_accuracy: 85.129%\n",
      "[013/00222] train_loss: 0.451\n",
      "[013/00272] train_loss: 0.485\n",
      "[013/00272] val_loss: 0.462, val_accuracy: 85.289%\n",
      "[013/00322] train_loss: 0.444\n",
      "[013/00372] train_loss: 0.471\n",
      "[013/00372] val_loss: 0.465, val_accuracy: 85.244%\n",
      "[013/00422] train_loss: 0.457\n",
      "[013/00472] train_loss: 0.480\n",
      "[013/00472] val_loss: 0.468, val_accuracy: 84.992%\n",
      "[014/00043] train_loss: 0.476\n",
      "[014/00093] train_loss: 0.483\n",
      "[014/00093] val_loss: 0.464, val_accuracy: 85.267%\n",
      "[014/00143] train_loss: 0.471\n",
      "[014/00193] train_loss: 0.445\n",
      "[014/00193] val_loss: 0.464, val_accuracy: 84.832%\n",
      "[014/00243] train_loss: 0.473\n",
      "[014/00293] train_loss: 0.474\n",
      "[014/00293] val_loss: 0.464, val_accuracy: 85.152%\n",
      "[014/00343] train_loss: 0.470\n",
      "[014/00393] train_loss: 0.447\n",
      "[014/00393] val_loss: 0.466, val_accuracy: 85.381%\n",
      "[014/00443] train_loss: 0.487\n",
      "[015/00014] train_loss: 0.463\n",
      "[015/00014] val_loss: 0.468, val_accuracy: 84.878%\n",
      "[015/00064] train_loss: 0.468\n",
      "[015/00114] train_loss: 0.480\n",
      "[015/00114] val_loss: 0.464, val_accuracy: 85.198%\n",
      "[015/00164] train_loss: 0.473\n",
      "[015/00214] train_loss: 0.481\n",
      "[015/00214] val_loss: 0.468, val_accuracy: 84.969%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[015/00264] train_loss: 0.446\n",
      "[015/00314] train_loss: 0.451\n",
      "[015/00314] val_loss: 0.469, val_accuracy: 84.923%\n",
      "[015/00364] train_loss: 0.489\n",
      "[015/00414] train_loss: 0.477\n",
      "[015/00414] val_loss: 0.465, val_accuracy: 85.198%\n",
      "[015/00464] train_loss: 0.450\n",
      "[016/00035] train_loss: 0.468\n",
      "[016/00035] val_loss: 0.463, val_accuracy: 84.992%\n",
      "[016/00085] train_loss: 0.514\n",
      "[016/00135] train_loss: 0.458\n",
      "[016/00135] val_loss: 0.467, val_accuracy: 85.038%\n",
      "[016/00185] train_loss: 0.471\n",
      "[016/00235] train_loss: 0.440\n",
      "[016/00235] val_loss: 0.464, val_accuracy: 85.289%\n",
      "[016/00285] train_loss: 0.463\n",
      "[016/00335] train_loss: 0.446\n",
      "[016/00335] val_loss: 0.462, val_accuracy: 85.152%\n",
      "[016/00385] train_loss: 0.481\n",
      "[016/00435] train_loss: 0.449\n",
      "[016/00435] val_loss: 0.468, val_accuracy: 85.221%\n",
      "[017/00006] train_loss: 0.423\n",
      "[017/00056] train_loss: 0.475\n",
      "[017/00056] val_loss: 0.463, val_accuracy: 85.152%\n",
      "[017/00106] train_loss: 0.482\n",
      "[017/00156] train_loss: 0.443\n",
      "[017/00156] val_loss: 0.467, val_accuracy: 85.084%\n",
      "[017/00206] train_loss: 0.442\n",
      "[017/00256] train_loss: 0.502\n",
      "[017/00256] val_loss: 0.467, val_accuracy: 84.878%\n",
      "[017/00306] train_loss: 0.433\n",
      "[017/00356] train_loss: 0.444\n",
      "[017/00356] val_loss: 0.469, val_accuracy: 85.015%\n",
      "[017/00406] train_loss: 0.473\n",
      "[017/00456] train_loss: 0.508\n",
      "[017/00456] val_loss: 0.468, val_accuracy: 84.878%\n",
      "[018/00027] train_loss: 0.469\n",
      "[018/00077] train_loss: 0.455\n",
      "[018/00077] val_loss: 0.461, val_accuracy: 85.061%\n",
      "[018/00127] train_loss: 0.487\n",
      "[018/00177] train_loss: 0.463\n",
      "[018/00177] val_loss: 0.462, val_accuracy: 85.175%\n",
      "[018/00227] train_loss: 0.476\n",
      "[018/00277] train_loss: 0.439\n",
      "[018/00277] val_loss: 0.467, val_accuracy: 85.335%\n",
      "[018/00327] train_loss: 0.465\n",
      "[018/00377] train_loss: 0.472\n",
      "[018/00377] val_loss: 0.463, val_accuracy: 85.084%\n",
      "[018/00427] train_loss: 0.459\n",
      "[018/00477] train_loss: 0.460\n",
      "[018/00477] val_loss: 0.465, val_accuracy: 85.175%\n",
      "[019/00048] train_loss: 0.468\n",
      "[019/00098] train_loss: 0.468\n",
      "[019/00098] val_loss: 0.464, val_accuracy: 85.472%\n",
      "[019/00148] train_loss: 0.451\n",
      "[019/00198] train_loss: 0.461\n",
      "[019/00198] val_loss: 0.466, val_accuracy: 85.129%\n",
      "[019/00248] train_loss: 0.459\n",
      "[019/00298] train_loss: 0.481\n",
      "[019/00298] val_loss: 0.465, val_accuracy: 85.198%\n",
      "[019/00348] train_loss: 0.457\n",
      "[019/00398] train_loss: 0.489\n",
      "[019/00398] val_loss: 0.465, val_accuracy: 85.221%\n",
      "[019/00448] train_loss: 0.441\n",
      "[020/00019] train_loss: 0.462\n",
      "[020/00019] val_loss: 0.461, val_accuracy: 84.946%\n",
      "[020/00069] train_loss: 0.455\n",
      "[020/00119] train_loss: 0.463\n",
      "[020/00119] val_loss: 0.465, val_accuracy: 84.923%\n",
      "[020/00169] train_loss: 0.453\n",
      "[020/00219] train_loss: 0.450\n",
      "[020/00219] val_loss: 0.468, val_accuracy: 84.855%\n",
      "[020/00269] train_loss: 0.468\n",
      "[020/00319] train_loss: 0.466\n",
      "[020/00319] val_loss: 0.470, val_accuracy: 85.015%\n",
      "[020/00369] train_loss: 0.458\n",
      "[020/00419] train_loss: 0.452\n",
      "[020/00419] val_loss: 0.467, val_accuracy: 84.946%\n",
      "[020/00469] train_loss: 0.467\n",
      "[021/00040] train_loss: 0.472\n",
      "[021/00040] val_loss: 0.468, val_accuracy: 84.740%\n",
      "[021/00090] train_loss: 0.478\n",
      "[021/00140] train_loss: 0.469\n",
      "[021/00140] val_loss: 0.463, val_accuracy: 85.221%\n",
      "[021/00190] train_loss: 0.471\n",
      "[021/00240] train_loss: 0.468\n",
      "[021/00240] val_loss: 0.471, val_accuracy: 84.969%\n",
      "[021/00290] train_loss: 0.454\n",
      "[021/00340] train_loss: 0.445\n",
      "[021/00340] val_loss: 0.461, val_accuracy: 85.358%\n",
      "[021/00390] train_loss: 0.463\n",
      "[021/00440] train_loss: 0.469\n",
      "[021/00440] val_loss: 0.469, val_accuracy: 84.900%\n",
      "[022/00011] train_loss: 0.451\n",
      "[022/00061] train_loss: 0.467\n",
      "[022/00061] val_loss: 0.462, val_accuracy: 84.878%\n",
      "[022/00111] train_loss: 0.446\n",
      "[022/00161] train_loss: 0.460\n",
      "[022/00161] val_loss: 0.465, val_accuracy: 85.038%\n",
      "[022/00211] train_loss: 0.480\n",
      "[022/00261] train_loss: 0.479\n",
      "[022/00261] val_loss: 0.467, val_accuracy: 85.015%\n",
      "[022/00311] train_loss: 0.464\n",
      "[022/00361] train_loss: 0.466\n",
      "[022/00361] val_loss: 0.465, val_accuracy: 85.061%\n",
      "[022/00411] train_loss: 0.458\n",
      "[022/00461] train_loss: 0.486\n",
      "[022/00461] val_loss: 0.459, val_accuracy: 85.312%\n",
      "[023/00032] train_loss: 0.469\n",
      "[023/00082] train_loss: 0.457\n",
      "[023/00082] val_loss: 0.465, val_accuracy: 85.152%\n",
      "[023/00132] train_loss: 0.460\n",
      "[023/00182] train_loss: 0.464\n",
      "[023/00182] val_loss: 0.465, val_accuracy: 85.106%\n",
      "[023/00232] train_loss: 0.495\n",
      "[023/00282] train_loss: 0.467\n",
      "[023/00282] val_loss: 0.468, val_accuracy: 85.106%\n",
      "[023/00332] train_loss: 0.446\n",
      "[023/00382] train_loss: 0.460\n",
      "[023/00382] val_loss: 0.467, val_accuracy: 85.015%\n",
      "[023/00432] train_loss: 0.473\n",
      "[024/00003] train_loss: 0.449\n",
      "[024/00003] val_loss: 0.467, val_accuracy: 84.809%\n",
      "[024/00053] train_loss: 0.461\n",
      "[024/00103] train_loss: 0.460\n",
      "[024/00103] val_loss: 0.468, val_accuracy: 84.946%\n",
      "[024/00153] train_loss: 0.456\n",
      "[024/00203] train_loss: 0.495\n",
      "[024/00203] val_loss: 0.464, val_accuracy: 85.152%\n",
      "[024/00253] train_loss: 0.450\n",
      "[024/00303] train_loss: 0.453\n",
      "[024/00303] val_loss: 0.464, val_accuracy: 85.152%\n",
      "[024/00353] train_loss: 0.477\n",
      "[024/00403] train_loss: 0.483\n",
      "[024/00403] val_loss: 0.468, val_accuracy: 85.015%\n",
      "[024/00453] train_loss: 0.474\n",
      "[025/00024] train_loss: 0.473\n",
      "[025/00024] val_loss: 0.463, val_accuracy: 85.106%\n",
      "[025/00074] train_loss: 0.462\n",
      "[025/00124] train_loss: 0.470\n",
      "[025/00124] val_loss: 0.470, val_accuracy: 84.809%\n",
      "[025/00174] train_loss: 0.479\n",
      "[025/00224] train_loss: 0.472\n",
      "[025/00224] val_loss: 0.466, val_accuracy: 84.763%\n",
      "[025/00274] train_loss: 0.445\n",
      "[025/00324] train_loss: 0.478\n",
      "[025/00324] val_loss: 0.465, val_accuracy: 85.358%\n",
      "[025/00374] train_loss: 0.487\n",
      "[025/00424] train_loss: 0.473\n",
      "[025/00424] val_loss: 0.463, val_accuracy: 85.358%\n",
      "[025/00474] train_loss: 0.434\n",
      "[026/00045] train_loss: 0.460\n",
      "[026/00045] val_loss: 0.465, val_accuracy: 85.084%\n",
      "[026/00095] train_loss: 0.475\n",
      "[026/00145] train_loss: 0.459\n",
      "[026/00145] val_loss: 0.468, val_accuracy: 85.038%\n",
      "[026/00195] train_loss: 0.447\n",
      "[026/00245] train_loss: 0.496\n",
      "[026/00245] val_loss: 0.464, val_accuracy: 85.015%\n",
      "[026/00295] train_loss: 0.459\n",
      "[026/00345] train_loss: 0.462\n",
      "[026/00345] val_loss: 0.464, val_accuracy: 85.335%\n",
      "[026/00395] train_loss: 0.467\n",
      "[026/00445] train_loss: 0.457\n",
      "[026/00445] val_loss: 0.465, val_accuracy: 84.832%\n",
      "[027/00016] train_loss: 0.488\n",
      "[027/00066] train_loss: 0.481\n",
      "[027/00066] val_loss: 0.468, val_accuracy: 84.969%\n",
      "[027/00116] train_loss: 0.461\n",
      "[027/00166] train_loss: 0.448\n",
      "[027/00166] val_loss: 0.463, val_accuracy: 85.129%\n",
      "[027/00216] train_loss: 0.467\n",
      "[027/00266] train_loss: 0.464\n",
      "[027/00266] val_loss: 0.463, val_accuracy: 85.335%\n",
      "[027/00316] train_loss: 0.489\n",
      "[027/00366] train_loss: 0.458\n",
      "[027/00366] val_loss: 0.464, val_accuracy: 85.244%\n",
      "[027/00416] train_loss: 0.464\n",
      "[027/00466] train_loss: 0.441\n",
      "[027/00466] val_loss: 0.467, val_accuracy: 85.106%\n",
      "[028/00037] train_loss: 0.468\n",
      "[028/00087] train_loss: 0.442\n",
      "[028/00087] val_loss: 0.468, val_accuracy: 85.175%\n",
      "[028/00137] train_loss: 0.458\n",
      "[028/00187] train_loss: 0.467\n",
      "[028/00187] val_loss: 0.464, val_accuracy: 85.198%\n",
      "[028/00237] train_loss: 0.453\n",
      "[028/00287] train_loss: 0.481\n",
      "[028/00287] val_loss: 0.465, val_accuracy: 85.244%\n",
      "[028/00337] train_loss: 0.473\n",
      "[028/00387] train_loss: 0.463\n",
      "[028/00387] val_loss: 0.468, val_accuracy: 85.221%\n",
      "[028/00437] train_loss: 0.469\n",
      "[029/00008] train_loss: 0.458\n",
      "[029/00008] val_loss: 0.462, val_accuracy: 84.969%\n",
      "[029/00058] train_loss: 0.489\n",
      "[029/00108] train_loss: 0.453\n",
      "[029/00108] val_loss: 0.469, val_accuracy: 84.946%\n",
      "[029/00158] train_loss: 0.451\n",
      "[029/00208] train_loss: 0.469\n",
      "[029/00208] val_loss: 0.466, val_accuracy: 85.427%\n",
      "[029/00258] train_loss: 0.463\n",
      "[029/00308] train_loss: 0.457\n",
      "[029/00308] val_loss: 0.465, val_accuracy: 84.992%\n",
      "[029/00358] train_loss: 0.468\n",
      "[029/00408] train_loss: 0.457\n",
      "[029/00408] val_loss: 0.467, val_accuracy: 84.900%\n",
      "[029/00458] train_loss: 0.466\n"
     ]
    }
   ],
   "source": [
    "# Finetune backbone: 30 epochs with small lr\n",
    "from mvcnn_rec.training import train_mvcnn\n",
    "config = {\n",
    "    'experiment_name': '1vcnn_generalize_finetune_backbone',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'resume_ckpt': 'mvcnn_rec/runs/1vcnn_generalize/model_best.ckpt',\n",
    "    'learning_rate': 0.0001,\n",
    "    'max_epochs': 30,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 100,\n",
    "    'num_views': 1,\n",
    "    'random_start_view': True, \n",
    "    'freezee_backbone': False\n",
    "}\n",
    "train_mvcnn.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb27fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 1.078\n",
      "[000/00099] train_loss: 0.579\n",
      "[000/00099] 2022-01-08 11:23:16.267280 val_loss: 0.620, val_accuracy: 80.737%\n",
      "[000/00149] train_loss: 0.539\n",
      "[000/00199] train_loss: 0.506\n",
      "[000/00199] 2022-01-08 11:23:26.176667 val_loss: 0.575, val_accuracy: 80.485%\n",
      "[000/00249] train_loss: 0.542\n",
      "[000/00299] train_loss: 0.454\n",
      "[000/00299] 2022-01-08 11:23:35.716679 val_loss: 0.432, val_accuracy: 86.342%\n",
      "[000/00349] train_loss: 0.438\n",
      "[000/00399] train_loss: 0.521\n",
      "[000/00399] 2022-01-08 11:23:45.420904 val_loss: 0.671, val_accuracy: 79.181%\n",
      "[000/00449] train_loss: 0.468\n",
      "[001/00020] train_loss: 0.463\n",
      "[001/00020] 2022-01-08 11:23:55.411970 val_loss: 0.862, val_accuracy: 73.530%\n",
      "[001/00070] train_loss: 0.481\n",
      "[001/00120] train_loss: 0.453\n",
      "[001/00120] 2022-01-08 11:24:05.114953 val_loss: 0.482, val_accuracy: 83.917%\n",
      "[001/00170] train_loss: 0.507\n",
      "[001/00220] train_loss: 0.455\n",
      "[001/00220] 2022-01-08 11:24:14.769860 val_loss: 0.809, val_accuracy: 73.576%\n",
      "[001/00270] train_loss: 0.427\n",
      "[001/00320] train_loss: 0.443\n",
      "[001/00320] 2022-01-08 11:24:24.380209 val_loss: 0.437, val_accuracy: 85.564%\n",
      "[001/00370] train_loss: 0.447\n",
      "[001/00420] train_loss: 0.428\n",
      "[001/00420] 2022-01-08 11:24:33.963242 val_loss: 0.518, val_accuracy: 82.384%\n",
      "[001/00470] train_loss: 0.430\n",
      "[002/00041] train_loss: 0.447\n",
      "[002/00041] 2022-01-08 11:24:44.080882 val_loss: 1.034, val_accuracy: 72.157%\n",
      "[002/00091] train_loss: 0.440\n",
      "[002/00141] train_loss: 0.421\n",
      "[002/00141] 2022-01-08 11:24:53.726712 val_loss: 0.539, val_accuracy: 83.116%\n",
      "[002/00191] train_loss: 0.459\n",
      "[002/00241] train_loss: 0.445\n",
      "[002/00241] 2022-01-08 11:25:03.331812 val_loss: 0.501, val_accuracy: 83.871%\n",
      "[002/00291] train_loss: 0.453\n",
      "[002/00341] train_loss: 0.429\n",
      "[002/00341] 2022-01-08 11:25:13.041181 val_loss: 0.476, val_accuracy: 84.008%\n",
      "[002/00391] train_loss: 0.413\n",
      "[002/00441] train_loss: 0.466\n",
      "[002/00441] 2022-01-08 11:25:22.701331 val_loss: 0.560, val_accuracy: 81.354%\n",
      "[003/00012] train_loss: 0.445\n",
      "[003/00062] train_loss: 0.455\n",
      "[003/00062] 2022-01-08 11:25:32.816154 val_loss: 0.677, val_accuracy: 78.220%\n",
      "[003/00112] train_loss: 0.425\n",
      "[003/00162] train_loss: 0.432\n",
      "[003/00162] 2022-01-08 11:25:42.465582 val_loss: 0.437, val_accuracy: 85.930%\n",
      "[003/00212] train_loss: 0.417\n",
      "[003/00262] train_loss: 0.433\n",
      "[003/00262] 2022-01-08 11:25:52.108246 val_loss: 0.540, val_accuracy: 82.910%\n",
      "[003/00312] train_loss: 0.457\n",
      "[003/00362] train_loss: 0.387\n",
      "[003/00362] 2022-01-08 11:26:01.801988 val_loss: 0.634, val_accuracy: 81.263%\n",
      "[003/00412] train_loss: 0.398\n",
      "[003/00462] train_loss: 0.462\n",
      "[003/00462] 2022-01-08 11:26:11.457201 val_loss: 0.445, val_accuracy: 85.541%\n",
      "[004/00033] train_loss: 0.422\n",
      "[004/00083] train_loss: 0.422\n",
      "[004/00083] 2022-01-08 11:26:21.452858 val_loss: 0.609, val_accuracy: 80.142%\n",
      "[004/00133] train_loss: 0.404\n",
      "[004/00183] train_loss: 0.408\n",
      "[004/00183] 2022-01-08 11:26:31.035730 val_loss: 1.008, val_accuracy: 67.879%\n",
      "[004/00233] train_loss: 0.420\n",
      "[004/00283] train_loss: 0.389\n",
      "[004/00283] 2022-01-08 11:26:40.838467 val_loss: 0.867, val_accuracy: 72.363%\n",
      "[004/00333] train_loss: 0.434\n",
      "[004/00383] train_loss: 0.425\n",
      "[004/00383] 2022-01-08 11:26:50.460803 val_loss: 0.640, val_accuracy: 79.684%\n",
      "[004/00433] train_loss: 0.436\n",
      "[005/00004] train_loss: 0.429\n",
      "[005/00004] 2022-01-08 11:27:00.395070 val_loss: 0.525, val_accuracy: 83.047%\n",
      "[005/00054] train_loss: 0.415\n",
      "[005/00104] train_loss: 0.421\n",
      "[005/00104] 2022-01-08 11:27:10.094139 val_loss: 0.519, val_accuracy: 83.528%\n",
      "[005/00154] train_loss: 0.402\n",
      "[005/00204] train_loss: 0.387\n",
      "[005/00204] 2022-01-08 11:27:19.713723 val_loss: 0.427, val_accuracy: 86.090%\n",
      "[005/00254] train_loss: 0.393\n",
      "[005/00304] train_loss: 0.371\n",
      "[005/00304] 2022-01-08 11:27:29.262901 val_loss: 0.558, val_accuracy: 83.116%\n",
      "[005/00354] train_loss: 0.417\n",
      "[005/00404] train_loss: 0.401\n",
      "[005/00404] 2022-01-08 11:27:38.860547 val_loss: 0.612, val_accuracy: 80.805%\n",
      "[005/00454] train_loss: 0.373\n",
      "[006/00025] train_loss: 0.426\n",
      "[006/00025] 2022-01-08 11:27:49.025014 val_loss: 0.524, val_accuracy: 83.688%\n",
      "[006/00075] train_loss: 0.406\n",
      "[006/00125] train_loss: 0.396\n",
      "[006/00125] 2022-01-08 11:27:58.732627 val_loss: 0.587, val_accuracy: 82.430%\n",
      "[006/00175] train_loss: 0.393\n",
      "[006/00225] train_loss: 0.389\n",
      "[006/00225] 2022-01-08 11:28:07.821972 val_loss: 0.791, val_accuracy: 76.092%\n",
      "[006/00275] train_loss: 0.421\n",
      "[006/00325] train_loss: 0.375\n",
      "[006/00325] 2022-01-08 11:28:16.372289 val_loss: 1.329, val_accuracy: 60.558%\n",
      "[006/00375] train_loss: 0.386\n",
      "[006/00425] train_loss: 0.384\n",
      "[006/00425] 2022-01-08 11:28:25.111110 val_loss: 0.460, val_accuracy: 85.106%\n",
      "[006/00475] train_loss: 0.383\n",
      "[007/00046] train_loss: 0.407\n",
      "[007/00046] 2022-01-08 11:28:34.329911 val_loss: 0.529, val_accuracy: 84.351%\n",
      "[007/00096] train_loss: 0.368\n",
      "[007/00146] train_loss: 0.383\n",
      "[007/00146] 2022-01-08 11:28:43.177652 val_loss: 0.633, val_accuracy: 81.492%\n",
      "[007/00196] train_loss: 0.374\n",
      "[007/00246] train_loss: 0.381\n",
      "[007/00246] 2022-01-08 11:28:51.936938 val_loss: 0.583, val_accuracy: 81.354%\n",
      "[007/00296] train_loss: 0.401\n",
      "[007/00346] train_loss: 0.412\n",
      "[007/00346] 2022-01-08 11:29:00.555186 val_loss: 1.966, val_accuracy: 38.389%\n",
      "[007/00396] train_loss: 0.370\n",
      "[007/00446] train_loss: 0.391\n",
      "[007/00446] 2022-01-08 11:29:10.207708 val_loss: 0.475, val_accuracy: 85.472%\n",
      "[008/00017] train_loss: 0.356\n",
      "[008/00067] train_loss: 0.361\n",
      "[008/00067] 2022-01-08 11:29:20.397117 val_loss: 0.395, val_accuracy: 87.783%\n",
      "[008/00117] train_loss: 0.374\n",
      "[008/00167] train_loss: 0.380\n",
      "[008/00167] 2022-01-08 11:29:30.390307 val_loss: 0.475, val_accuracy: 84.786%\n",
      "[008/00217] train_loss: 0.369\n",
      "[008/00267] train_loss: 0.363\n",
      "[008/00267] 2022-01-08 11:29:40.094632 val_loss: 0.839, val_accuracy: 74.216%\n",
      "[008/00317] train_loss: 0.372\n",
      "[008/00367] train_loss: 0.392\n",
      "[008/00367] 2022-01-08 11:29:49.751276 val_loss: 0.538, val_accuracy: 82.567%\n",
      "[008/00417] train_loss: 0.398\n",
      "[008/00467] train_loss: 0.371\n",
      "[008/00467] 2022-01-08 11:29:59.338653 val_loss: 0.375, val_accuracy: 88.264%\n",
      "[009/00038] train_loss: 0.379\n",
      "[009/00088] train_loss: 0.375\n",
      "[009/00088] 2022-01-08 11:30:09.679265 val_loss: 0.375, val_accuracy: 87.875%\n",
      "[009/00138] train_loss: 0.350\n",
      "[009/00188] train_loss: 0.344\n",
      "[009/00188] 2022-01-08 11:30:19.510806 val_loss: 0.465, val_accuracy: 85.404%\n",
      "[009/00238] train_loss: 0.373\n",
      "[009/00288] train_loss: 0.375\n",
      "[009/00288] 2022-01-08 11:30:29.198947 val_loss: 0.579, val_accuracy: 82.567%\n",
      "[009/00338] train_loss: 0.367\n",
      "[009/00388] train_loss: 0.387\n",
      "[009/00388] 2022-01-08 11:30:38.982133 val_loss: 0.419, val_accuracy: 86.593%\n",
      "[009/00438] train_loss: 0.379\n",
      "[010/00009] train_loss: 0.355\n",
      "[010/00009] 2022-01-08 11:30:49.015333 val_loss: 0.428, val_accuracy: 86.388%\n",
      "[010/00059] train_loss: 0.381\n",
      "[010/00109] train_loss: 0.362\n",
      "[012/00401] train_loss: 0.378\n",
      "[012/00451] train_loss: 0.329\n",
      "[012/00451] 2022-01-08 11:33:05.109486 val_loss: 0.377, val_accuracy: 88.149%\n",
      "[013/00022] train_loss: 0.366\n",
      "[013/00072] train_loss: 0.362\n",
      "[013/00072] 2022-01-08 11:33:15.138028 val_loss: 0.350, val_accuracy: 89.110%\n",
      "[013/00122] train_loss: 0.355\n",
      "[013/00172] train_loss: 0.347\n",
      "[013/00172] 2022-01-08 11:33:24.993893 val_loss: 0.650, val_accuracy: 78.746%\n",
      "[013/00222] train_loss: 0.350\n",
      "[013/00272] train_loss: 0.337\n",
      "[013/00272] 2022-01-08 11:33:34.553102 val_loss: 0.431, val_accuracy: 86.182%\n",
      "[013/00322] train_loss: 0.360\n",
      "[013/00372] train_loss: 0.343\n",
      "[013/00372] 2022-01-08 11:33:44.157076 val_loss: 0.363, val_accuracy: 88.698%\n",
      "[013/00422] train_loss: 0.357\n",
      "[013/00472] train_loss: 0.359\n",
      "[013/00472] 2022-01-08 11:33:53.735796 val_loss: 0.387, val_accuracy: 87.531%\n",
      "[014/00043] train_loss: 0.373\n",
      "[014/00093] train_loss: 0.341\n",
      "[014/00093] 2022-01-08 11:34:03.701926 val_loss: 0.431, val_accuracy: 86.388%\n",
      "[014/00143] train_loss: 0.336\n",
      "[014/00193] train_loss: 0.334\n",
      "[014/00193] 2022-01-08 11:34:13.357822 val_loss: 0.437, val_accuracy: 86.616%\n",
      "[014/00243] train_loss: 0.335\n",
      "[014/00293] train_loss: 0.364\n",
      "[014/00293] 2022-01-08 11:34:22.968807 val_loss: 0.369, val_accuracy: 88.378%\n",
      "[014/00343] train_loss: 0.382\n",
      "[014/00393] train_loss: 0.326\n",
      "[014/00393] 2022-01-08 11:34:32.725921 val_loss: 0.365, val_accuracy: 88.927%\n",
      "[014/00443] train_loss: 0.361\n",
      "[015/00014] train_loss: 0.357\n",
      "[015/00014] 2022-01-08 11:34:41.833423 val_loss: 0.345, val_accuracy: 88.790%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[015/00064] train_loss: 0.344\n",
      "[015/00114] train_loss: 0.351\n",
      "[015/00114] 2022-01-08 11:34:50.695683 val_loss: 0.349, val_accuracy: 89.179%\n",
      "[015/00164] train_loss: 0.327\n",
      "[015/00214] train_loss: 0.338\n",
      "[015/00214] 2022-01-08 11:34:59.717901 val_loss: 0.633, val_accuracy: 79.890%\n",
      "[015/00264] train_loss: 0.358\n",
      "[015/00314] train_loss: 0.355\n",
      "[015/00314] 2022-01-08 11:35:08.468677 val_loss: 1.879, val_accuracy: 54.816%\n",
      "[015/00364] train_loss: 0.347\n",
      "[015/00414] train_loss: 0.336\n",
      "[015/00414] 2022-01-08 11:35:17.291542 val_loss: 0.414, val_accuracy: 87.257%\n",
      "[015/00464] train_loss: 0.336\n",
      "[016/00035] train_loss: 0.361\n",
      "[016/00035] 2022-01-08 11:35:26.256488 val_loss: 0.408, val_accuracy: 87.303%\n",
      "[016/00085] train_loss: 0.344\n",
      "[016/00135] train_loss: 0.357\n",
      "[016/00135] 2022-01-08 11:35:35.271672 val_loss: 0.415, val_accuracy: 87.257%\n",
      "[016/00185] train_loss: 0.343\n",
      "[016/00235] train_loss: 0.338\n",
      "[016/00235] 2022-01-08 11:35:45.048580 val_loss: 0.375, val_accuracy: 88.378%\n",
      "[016/00285] train_loss: 0.337\n",
      "[016/00335] train_loss: 0.339\n",
      "[016/00335] 2022-01-08 11:35:54.582710 val_loss: 0.648, val_accuracy: 79.250%\n",
      "[016/00385] train_loss: 0.331\n",
      "[016/00435] train_loss: 0.325\n",
      "[016/00435] 2022-01-08 11:36:04.142276 val_loss: 0.456, val_accuracy: 85.495%\n",
      "[017/00006] train_loss: 0.337\n",
      "[017/00056] train_loss: 0.349\n",
      "[017/00056] 2022-01-08 11:36:14.341565 val_loss: 0.369, val_accuracy: 88.264%\n",
      "[017/00106] train_loss: 0.327\n",
      "[017/00156] train_loss: 0.321\n",
      "[017/00156] 2022-01-08 11:36:23.946280 val_loss: 0.408, val_accuracy: 87.005%\n",
      "[017/00206] train_loss: 0.365\n",
      "[017/00256] train_loss: 0.334\n",
      "[017/00256] 2022-01-08 11:36:33.549111 val_loss: 0.406, val_accuracy: 87.577%\n",
      "[017/00306] train_loss: 0.346\n",
      "[017/00356] train_loss: 0.332\n",
      "[017/00356] 2022-01-08 11:36:43.180438 val_loss: 0.377, val_accuracy: 87.829%\n",
      "[017/00406] train_loss: 0.313\n",
      "[017/00456] train_loss: 0.352\n",
      "[017/00456] 2022-01-08 11:36:52.837805 val_loss: 0.669, val_accuracy: 78.746%\n",
      "[018/00027] train_loss: 0.331\n",
      "[018/00077] train_loss: 0.337\n",
      "[018/00077] 2022-01-08 11:37:02.999693 val_loss: 0.471, val_accuracy: 85.221%\n",
      "[018/00127] train_loss: 0.345\n",
      "[018/00177] train_loss: 0.365\n",
      "[018/00177] 2022-01-08 11:37:12.649139 val_loss: 0.362, val_accuracy: 88.698%\n",
      "[018/00227] train_loss: 0.344\n",
      "[018/00277] train_loss: 0.350\n",
      "[018/00277] 2022-01-08 11:37:22.417939 val_loss: 0.359, val_accuracy: 88.035%\n",
      "[018/00327] train_loss: 0.321\n",
      "[018/00377] train_loss: 0.306\n",
      "[018/00377] 2022-01-08 11:37:32.135200 val_loss: 0.365, val_accuracy: 88.378%\n",
      "[018/00427] train_loss: 0.353\n",
      "[018/00477] train_loss: 0.320\n",
      "[018/00477] 2022-01-08 11:37:41.666180 val_loss: 0.381, val_accuracy: 87.440%\n",
      "[019/00048] train_loss: 0.331\n",
      "[019/00098] train_loss: 0.326\n",
      "[019/00098] 2022-01-08 11:37:51.620363 val_loss: 2.172, val_accuracy: 49.760%\n",
      "[019/00148] train_loss: 0.330\n",
      "[019/00198] train_loss: 0.335\n",
      "[019/00198] 2022-01-08 11:38:00.974766 val_loss: 0.412, val_accuracy: 86.982%\n",
      "[019/00248] train_loss: 0.325\n",
      "[019/00298] train_loss: 0.334\n",
      "[019/00298] 2022-01-08 11:38:10.711357 val_loss: 0.685, val_accuracy: 79.707%\n",
      "[019/00348] train_loss: 0.344\n",
      "[019/00398] train_loss: 0.362\n",
      "[019/00398] 2022-01-08 11:38:20.234971 val_loss: 0.345, val_accuracy: 88.996%\n",
      "[019/00448] train_loss: 0.341\n",
      "[020/00019] train_loss: 0.338\n",
      "[020/00019] 2022-01-08 11:38:30.214517 val_loss: 0.408, val_accuracy: 87.623%\n",
      "[020/00069] train_loss: 0.329\n",
      "[020/00119] train_loss: 0.353\n",
      "[020/00119] 2022-01-08 11:38:39.836416 val_loss: 0.357, val_accuracy: 88.790%\n",
      "[020/00169] train_loss: 0.348\n",
      "[020/00219] train_loss: 0.321\n",
      "[020/00219] 2022-01-08 11:38:49.331014 val_loss: 0.371, val_accuracy: 88.172%\n",
      "[020/00269] train_loss: 0.313\n",
      "[020/00319] train_loss: 0.331\n",
      "[020/00319] 2022-01-08 11:38:59.064991 val_loss: 0.375, val_accuracy: 88.996%\n",
      "[020/00369] train_loss: 0.332\n",
      "[020/00419] train_loss: 0.326\n",
      "[020/00419] 2022-01-08 11:39:08.619986 val_loss: 0.341, val_accuracy: 89.270%\n",
      "[020/00469] train_loss: 0.337\n",
      "[021/00040] train_loss: 0.319\n",
      "[021/00040] 2022-01-08 11:39:18.788361 val_loss: 0.402, val_accuracy: 87.097%\n",
      "[021/00090] train_loss: 0.311\n",
      "[021/00140] train_loss: 0.342\n",
      "[021/00140] 2022-01-08 11:39:28.340853 val_loss: 0.441, val_accuracy: 85.816%\n",
      "[021/00190] train_loss: 0.323\n",
      "[021/00240] train_loss: 0.323\n",
      "[021/00240] 2022-01-08 11:39:38.034015 val_loss: 0.339, val_accuracy: 88.927%\n",
      "[021/00290] train_loss: 0.337\n",
      "[021/00340] train_loss: 0.319\n",
      "[021/00340] 2022-01-08 11:39:47.607640 val_loss: 0.658, val_accuracy: 80.233%\n",
      "[021/00390] train_loss: 0.311\n",
      "[021/00440] train_loss: 0.322\n",
      "[021/00440] 2022-01-08 11:39:57.345672 val_loss: 0.390, val_accuracy: 87.326%\n",
      "[022/00011] train_loss: 0.348\n",
      "[022/00061] train_loss: 0.323\n",
      "[022/00061] 2022-01-08 11:40:07.484134 val_loss: 0.403, val_accuracy: 87.898%\n",
      "[022/00111] train_loss: 0.329\n",
      "[022/00161] train_loss: 0.313\n",
      "[022/00161] 2022-01-08 11:40:17.116206 val_loss: 0.526, val_accuracy: 84.146%\n",
      "[022/00211] train_loss: 0.321\n",
      "[022/00261] train_loss: 0.341\n",
      "[022/00261] 2022-01-08 11:40:26.686374 val_loss: 0.553, val_accuracy: 82.658%\n",
      "[022/00311] train_loss: 0.326\n",
      "[022/00361] train_loss: 0.282\n",
      "[022/00361] 2022-01-08 11:40:36.298706 val_loss: 0.367, val_accuracy: 87.783%\n",
      "[022/00411] train_loss: 0.334\n",
      "[022/00461] train_loss: 0.323\n",
      "[022/00461] 2022-01-08 11:40:45.825889 val_loss: 0.409, val_accuracy: 86.845%\n",
      "[023/00032] train_loss: 0.338\n",
      "[023/00082] train_loss: 0.292\n",
      "[023/00082] 2022-01-08 11:40:55.714324 val_loss: 0.354, val_accuracy: 88.744%\n",
      "[023/00132] train_loss: 0.342\n",
      "[023/00182] train_loss: 0.338\n",
      "[023/00182] 2022-01-08 11:41:05.262286 val_loss: 0.389, val_accuracy: 88.035%\n",
      "[023/00232] train_loss: 0.354\n",
      "[023/00282] train_loss: 0.317\n",
      "[023/00282] 2022-01-08 11:41:13.908071 val_loss: 0.512, val_accuracy: 83.482%\n",
      "[023/00332] train_loss: 0.293\n",
      "[023/00382] train_loss: 0.333\n",
      "[023/00382] 2022-01-08 11:41:22.829661 val_loss: 0.362, val_accuracy: 88.515%\n",
      "[023/00432] train_loss: 0.299\n",
      "[024/00003] train_loss: 0.351\n",
      "[024/00003] 2022-01-08 11:41:31.847366 val_loss: 0.354, val_accuracy: 88.767%\n",
      "[024/00053] train_loss: 0.347\n",
      "[024/00103] train_loss: 0.328\n",
      "[024/00103] 2022-01-08 11:41:40.408682 val_loss: 0.373, val_accuracy: 88.652%\n",
      "[024/00153] train_loss: 0.320\n",
      "[024/00203] train_loss: 0.349\n",
      "[024/00203] 2022-01-08 11:41:49.307104 val_loss: 0.394, val_accuracy: 87.554%\n",
      "[024/00253] train_loss: 0.324\n",
      "[024/00303] train_loss: 0.342\n",
      "[024/00303] 2022-01-08 11:41:57.894685 val_loss: 0.340, val_accuracy: 89.568%\n",
      "[024/00353] train_loss: 0.313\n",
      "[024/00403] train_loss: 0.325\n",
      "[024/00403] 2022-01-08 11:42:07.161811 val_loss: 0.337, val_accuracy: 89.202%\n",
      "[024/00453] train_loss: 0.294\n",
      "[025/00024] train_loss: 0.333\n",
      "[025/00024] 2022-01-08 11:42:17.236098 val_loss: 0.373, val_accuracy: 88.332%\n",
      "[025/00074] train_loss: 0.300\n",
      "[025/00124] train_loss: 0.312\n",
      "[025/00124] 2022-01-08 11:42:26.805308 val_loss: 0.358, val_accuracy: 88.652%\n",
      "[025/00174] train_loss: 0.339\n",
      "[025/00224] train_loss: 0.348\n",
      "[025/00224] 2022-01-08 11:42:36.490928 val_loss: 0.662, val_accuracy: 78.197%\n",
      "[025/00274] train_loss: 0.337\n",
      "[025/00324] train_loss: 0.318\n",
      "[025/00324] 2022-01-08 11:42:46.055447 val_loss: 0.376, val_accuracy: 87.760%\n",
      "[025/00374] train_loss: 0.310\n",
      "[025/00424] train_loss: 0.321\n",
      "[025/00424] 2022-01-08 11:42:55.592937 val_loss: 0.343, val_accuracy: 88.927%\n",
      "[025/00474] train_loss: 0.312\n",
      "[026/00045] train_loss: 0.294\n",
      "[026/00045] 2022-01-08 11:43:05.621934 val_loss: 0.350, val_accuracy: 88.836%\n",
      "[026/00095] train_loss: 0.318\n",
      "[026/00145] train_loss: 0.315\n",
      "[026/00145] 2022-01-08 11:43:15.247815 val_loss: 0.345, val_accuracy: 89.430%\n",
      "[026/00195] train_loss: 0.344\n",
      "[026/00245] train_loss: 0.324\n",
      "[026/00245] 2022-01-08 11:43:24.830786 val_loss: 0.361, val_accuracy: 88.836%\n",
      "[026/00295] train_loss: 0.329\n",
      "[026/00345] train_loss: 0.356\n",
      "[026/00345] 2022-01-08 11:43:34.361109 val_loss: 0.446, val_accuracy: 85.976%\n",
      "[026/00395] train_loss: 0.313\n",
      "[026/00445] train_loss: 0.334\n",
      "[026/00445] 2022-01-08 11:43:43.913273 val_loss: 0.337, val_accuracy: 89.041%\n",
      "[027/00016] train_loss: 0.335\n",
      "[027/00066] train_loss: 0.328\n",
      "[027/00066] 2022-01-08 11:43:53.922354 val_loss: 0.412, val_accuracy: 86.639%\n",
      "[027/00116] train_loss: 0.315\n",
      "[027/00166] train_loss: 0.305\n",
      "[027/00166] 2022-01-08 11:44:03.506798 val_loss: 0.345, val_accuracy: 89.179%\n",
      "[027/00216] train_loss: 0.345\n",
      "[027/00266] train_loss: 0.319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[027/00266] 2022-01-08 11:44:13.175522 val_loss: 0.341, val_accuracy: 88.698%\n",
      "[027/00316] train_loss: 0.323\n",
      "[027/00366] train_loss: 0.333\n",
      "[027/00366] 2022-01-08 11:44:22.774620 val_loss: 0.326, val_accuracy: 89.430%\n",
      "[027/00416] train_loss: 0.339\n",
      "[027/00466] train_loss: 0.291\n",
      "[027/00466] 2022-01-08 11:44:32.422005 val_loss: 0.333, val_accuracy: 89.339%\n",
      "[028/00037] train_loss: 0.296\n",
      "[028/00087] train_loss: 0.325\n",
      "[028/00087] 2022-01-08 11:44:42.307324 val_loss: 0.389, val_accuracy: 87.737%\n",
      "[028/00137] train_loss: 0.312\n",
      "[028/00187] train_loss: 0.346\n",
      "[028/00187] 2022-01-08 11:44:51.877571 val_loss: 0.422, val_accuracy: 86.822%\n",
      "[028/00237] train_loss: 0.317\n",
      "[028/00287] train_loss: 0.336\n",
      "[028/00287] 2022-01-08 11:45:01.610764 val_loss: 0.364, val_accuracy: 88.515%\n",
      "[028/00337] train_loss: 0.329\n",
      "[028/00387] train_loss: 0.325\n",
      "[028/00387] 2022-01-08 11:45:11.267508 val_loss: 0.372, val_accuracy: 88.195%\n",
      "[028/00437] train_loss: 0.324\n",
      "[029/00008] train_loss: 0.326\n",
      "[029/00008] 2022-01-08 11:45:21.439699 val_loss: 0.335, val_accuracy: 88.721%\n",
      "[029/00058] train_loss: 0.304\n",
      "[029/00108] train_loss: 0.322\n",
      "[029/00108] 2022-01-08 11:45:31.192810 val_loss: 0.375, val_accuracy: 87.806%\n",
      "[029/00158] train_loss: 0.297\n",
      "[029/00208] train_loss: 0.337\n",
      "[029/00208] 2022-01-08 11:45:40.805354 val_loss: 0.484, val_accuracy: 85.312%\n",
      "[029/00258] train_loss: 0.312\n",
      "[029/00308] train_loss: 0.327\n",
      "[029/00308] 2022-01-08 11:45:50.372623 val_loss: 0.314, val_accuracy: 90.002%\n",
      "[029/00358] train_loss: 0.324\n",
      "[029/00408] train_loss: 0.321\n",
      "[029/00408] 2022-01-08 11:46:00.015256 val_loss: 0.419, val_accuracy: 86.754%\n",
      "[029/00458] train_loss: 0.329\n",
      "Best val_accuracy: 86.754%\n"
     ]
    }
   ],
   "source": [
    "# More 30 epochs with higher lr\n",
    "from mvcnn_rec.training import train_mvcnn\n",
    "config = {\n",
    "    'experiment_name': '1vcnn_generalize_finetune_backbone',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'resume_ckpt': 'mvcnn_rec/runs/1vcnn_generalize_finetune_backbone/model_best.ckpt',\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 30,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 100,\n",
    "    'num_views': 1,\n",
    "    'random_start_view': True, \n",
    "    'freezee_backbone': False\n",
    "}\n",
    "train_mvcnn.main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd855358",
   "metadata": {},
   "source": [
    "#### Finetuned give better results. Accuracy to 90%, val loss to 0.314"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f26fe16",
   "metadata": {},
   "source": [
    "#### Test finetune backbone from original pretrained resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc8df58b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 0.876\n",
      "[000/00099] train_loss: 0.640\n",
      "Num batch val 68\n",
      "[000/00099] 2022-01-08 15:35:10.162568 val_loss: 1.002, val_accuracy: 68.543%\n",
      "[000/00149] train_loss: 0.549\n",
      "[000/00199] train_loss: 0.541\n",
      "Num batch val 68\n",
      "[000/00199] 2022-01-08 15:35:18.115786 val_loss: 0.748, val_accuracy: 77.236%\n",
      "[000/00249] train_loss: 0.555\n",
      "[000/00299] train_loss: 0.572\n",
      "Num batch val 68\n",
      "[000/00299] 2022-01-08 15:35:26.191692 val_loss: 0.797, val_accuracy: 76.253%\n",
      "[000/00349] train_loss: 0.520\n",
      "[000/00399] train_loss: 0.515\n",
      "Num batch val 68\n",
      "[000/00399] 2022-01-08 15:35:34.096337 val_loss: 0.715, val_accuracy: 79.021%\n",
      "[000/00449] train_loss: 0.567\n",
      "[001/00020] train_loss: 0.482\n",
      "Num batch val 68\n",
      "[001/00020] 2022-01-08 15:35:42.697681 val_loss: 0.785, val_accuracy: 75.589%\n",
      "[001/00070] train_loss: 0.484\n",
      "[001/00120] train_loss: 0.451\n",
      "Num batch val 68\n",
      "[001/00120] 2022-01-08 15:35:50.546646 val_loss: 0.668, val_accuracy: 79.295%\n",
      "[001/00170] train_loss: 0.498\n",
      "[001/00220] train_loss: 0.510\n",
      "Num batch val 68\n",
      "[001/00220] 2022-01-08 15:35:58.648899 val_loss: 0.663, val_accuracy: 77.968%\n",
      "[001/00270] train_loss: 0.462\n",
      "[001/00320] train_loss: 0.491\n",
      "Num batch val 68\n",
      "[001/00320] 2022-01-08 15:36:06.589202 val_loss: 0.979, val_accuracy: 71.540%\n",
      "[001/00370] train_loss: 0.478\n",
      "[001/00420] train_loss: 0.448\n",
      "Num batch val 68\n",
      "[001/00420] 2022-01-08 15:36:14.478519 val_loss: 0.629, val_accuracy: 80.737%\n",
      "[001/00470] train_loss: 0.446\n",
      "[002/00041] train_loss: 0.468\n",
      "Num batch val 68\n",
      "[002/00041] 2022-01-08 15:36:22.998825 val_loss: 0.639, val_accuracy: 79.158%\n",
      "[002/00091] train_loss: 0.476\n",
      "[002/00141] train_loss: 0.466\n",
      "Num batch val 68\n",
      "[002/00141] 2022-01-08 15:36:30.823119 val_loss: 1.177, val_accuracy: 63.143%\n",
      "[002/00191] train_loss: 0.477\n",
      "[002/00241] train_loss: 0.443\n",
      "Num batch val 68\n",
      "[002/00241] 2022-01-08 15:36:38.689722 val_loss: 1.177, val_accuracy: 61.496%\n",
      "[002/00291] train_loss: 0.474\n",
      "[002/00341] train_loss: 0.408\n",
      "Num batch val 68\n",
      "[002/00341] 2022-01-08 15:36:46.379921 val_loss: 0.551, val_accuracy: 82.933%\n",
      "[002/00391] train_loss: 0.423\n",
      "[002/00441] train_loss: 0.451\n",
      "Num batch val 68\n",
      "[002/00441] 2022-01-08 15:36:54.429841 val_loss: 0.532, val_accuracy: 83.322%\n",
      "[003/00012] train_loss: 0.444\n",
      "[003/00062] train_loss: 0.422\n",
      "Num batch val 68\n",
      "[003/00062] 2022-01-08 15:37:02.989353 val_loss: 0.593, val_accuracy: 81.629%\n",
      "[003/00112] train_loss: 0.435\n",
      "[003/00162] train_loss: 0.454\n",
      "Num batch val 68\n",
      "[003/00162] 2022-01-08 15:37:10.968860 val_loss: 1.039, val_accuracy: 68.611%\n",
      "[003/00212] train_loss: 0.443\n",
      "[003/00262] train_loss: 0.419\n",
      "Num batch val 68\n",
      "[003/00262] 2022-01-08 15:37:18.814863 val_loss: 0.849, val_accuracy: 73.210%\n",
      "[003/00312] train_loss: 0.411\n",
      "[003/00362] train_loss: 0.442\n",
      "Num batch val 68\n",
      "[003/00362] 2022-01-08 15:37:26.604942 val_loss: 0.760, val_accuracy: 76.458%\n",
      "[003/00412] train_loss: 0.414\n",
      "[003/00462] train_loss: 0.420\n",
      "Num batch val 68\n",
      "[003/00462] 2022-01-08 15:37:34.449095 val_loss: 0.504, val_accuracy: 84.123%\n",
      "[004/00033] train_loss: 0.412\n",
      "[004/00083] train_loss: 0.433\n",
      "Num batch val 68\n",
      "[004/00083] 2022-01-08 15:37:42.938417 val_loss: 0.490, val_accuracy: 85.198%\n",
      "[004/00133] train_loss: 0.453\n",
      "[004/00183] train_loss: 0.411\n",
      "Num batch val 68\n",
      "[004/00183] 2022-01-08 15:37:51.003490 val_loss: 0.427, val_accuracy: 86.319%\n",
      "[004/00233] train_loss: 0.425\n",
      "[004/00283] train_loss: 0.443\n",
      "Num batch val 68\n",
      "[004/00283] 2022-01-08 15:37:59.074478 val_loss: 0.545, val_accuracy: 83.391%\n",
      "[004/00333] train_loss: 0.364\n",
      "[004/00383] train_loss: 0.392\n",
      "Num batch val 68\n",
      "[004/00383] 2022-01-08 15:38:06.965419 val_loss: 1.381, val_accuracy: 62.732%\n",
      "[004/00433] train_loss: 0.378\n",
      "[005/00004] train_loss: 0.412\n",
      "Num batch val 68\n",
      "[005/00004] 2022-01-08 15:38:15.311422 val_loss: 0.486, val_accuracy: 84.351%\n",
      "[005/00054] train_loss: 0.399\n",
      "[005/00104] train_loss: 0.413\n",
      "Num batch val 68\n",
      "[005/00104] 2022-01-08 15:38:23.125573 val_loss: 0.514, val_accuracy: 83.070%\n",
      "[005/00154] train_loss: 0.396\n",
      "[005/00204] train_loss: 0.397\n",
      "Num batch val 68\n",
      "[005/00204] 2022-01-08 15:38:30.956497 val_loss: 0.548, val_accuracy: 82.224%\n",
      "[005/00254] train_loss: 0.409\n",
      "[005/00304] train_loss: 0.416\n",
      "Num batch val 68\n",
      "[005/00304] 2022-01-08 15:38:38.834903 val_loss: 0.870, val_accuracy: 74.788%\n",
      "[005/00354] train_loss: 0.416\n",
      "[005/00404] train_loss: 0.402\n",
      "Num batch val 68\n",
      "[005/00404] 2022-01-08 15:38:46.758253 val_loss: 0.898, val_accuracy: 71.677%\n",
      "[005/00454] train_loss: 0.380\n",
      "[006/00025] train_loss: 0.386\n",
      "Num batch val 68\n",
      "[006/00025] 2022-01-08 15:38:55.156251 val_loss: 0.495, val_accuracy: 84.420%\n",
      "[006/00075] train_loss: 0.398\n",
      "[006/00125] train_loss: 0.374\n",
      "Num batch val 68\n",
      "[006/00125] 2022-01-08 15:39:03.116089 val_loss: 0.420, val_accuracy: 86.273%\n",
      "[006/00175] train_loss: 0.376\n",
      "[006/00225] train_loss: 0.369\n",
      "Num batch val 68\n",
      "[006/00225] 2022-01-08 15:39:10.951025 val_loss: 0.460, val_accuracy: 85.816%\n",
      "[006/00275] train_loss: 0.386\n",
      "[006/00325] train_loss: 0.403\n",
      "Num batch val 68\n",
      "[006/00325] 2022-01-08 15:39:18.854625 val_loss: 0.635, val_accuracy: 79.913%\n",
      "[006/00375] train_loss: 0.391\n",
      "[006/00425] train_loss: 0.386\n",
      "Num batch val 68\n",
      "[006/00425] 2022-01-08 15:39:26.753766 val_loss: 0.489, val_accuracy: 85.175%\n",
      "[006/00475] train_loss: 0.378\n",
      "[007/00046] train_loss: 0.387\n",
      "Num batch val 68\n",
      "[007/00046] 2022-01-08 15:39:35.116143 val_loss: 0.446, val_accuracy: 86.182%\n",
      "[007/00096] train_loss: 0.377\n",
      "[007/00146] train_loss: 0.396\n",
      "Num batch val 68\n",
      "[007/00146] 2022-01-08 15:39:43.058927 val_loss: 0.498, val_accuracy: 84.580%\n",
      "[007/00196] train_loss: 0.355\n",
      "[007/00246] train_loss: 0.398\n",
      "Num batch val 68\n",
      "[007/00246] 2022-01-08 15:39:50.865493 val_loss: 0.489, val_accuracy: 85.541%\n",
      "[007/00296] train_loss: 0.370\n",
      "[007/00346] train_loss: 0.409\n",
      "Num batch val 68\n",
      "[007/00346] 2022-01-08 15:39:58.782826 val_loss: 0.816, val_accuracy: 75.452%\n",
      "[007/00396] train_loss: 0.385\n",
      "[007/00446] train_loss: 0.362\n",
      "Num batch val 68\n",
      "[007/00446] 2022-01-08 15:40:06.603217 val_loss: 0.409, val_accuracy: 86.456%\n",
      "[008/00017] train_loss: 0.403\n",
      "[008/00067] train_loss: 0.367\n",
      "Num batch val 68\n",
      "[008/00067] 2022-01-08 15:40:15.006891 val_loss: 0.391, val_accuracy: 87.692%\n",
      "[008/00117] train_loss: 0.361\n",
      "[008/00167] train_loss: 0.378\n",
      "Num batch val 68\n",
      "[008/00167] 2022-01-08 15:40:22.923546 val_loss: 0.420, val_accuracy: 86.868%\n",
      "[008/00217] train_loss: 0.383\n",
      "[008/00267] train_loss: 0.377\n",
      "Num batch val 68\n",
      "[008/00267] 2022-01-08 15:40:30.617142 val_loss: 0.445, val_accuracy: 85.884%\n",
      "[008/00317] train_loss: 0.370\n",
      "[008/00367] train_loss: 0.381\n",
      "Num batch val 68\n",
      "[008/00367] 2022-01-08 15:40:38.419056 val_loss: 0.526, val_accuracy: 83.596%\n",
      "[008/00417] train_loss: 0.366\n",
      "[008/00467] train_loss: 0.359\n",
      "Num batch val 68\n",
      "[008/00467] 2022-01-08 15:40:46.310279 val_loss: 0.436, val_accuracy: 86.044%\n",
      "[009/00038] train_loss: 0.363\n",
      "[009/00088] train_loss: 0.389\n",
      "Num batch val 68\n",
      "[009/00088] 2022-01-08 15:40:54.689375 val_loss: 0.453, val_accuracy: 84.809%\n",
      "[009/00138] train_loss: 0.346\n",
      "[009/00188] train_loss: 0.356\n",
      "Num batch val 68\n",
      "[009/00188] 2022-01-08 15:41:02.489587 val_loss: 0.374, val_accuracy: 88.469%\n",
      "[009/00238] train_loss: 0.360\n",
      "[009/00288] train_loss: 0.387\n",
      "Num batch val 68\n",
      "[009/00288] 2022-01-08 15:41:10.558870 val_loss: 0.504, val_accuracy: 83.734%\n",
      "[009/00338] train_loss: 0.374\n",
      "[009/00388] train_loss: 0.386\n",
      "Num batch val 68\n",
      "[009/00388] 2022-01-08 15:41:18.473949 val_loss: 0.457, val_accuracy: 85.312%\n",
      "[009/00438] train_loss: 0.356\n",
      "[010/00009] train_loss: 0.352\n",
      "Num batch val 68\n",
      "[010/00009] 2022-01-08 15:41:26.762508 val_loss: 0.407, val_accuracy: 86.982%\n",
      "[010/00059] train_loss: 0.345\n",
      "[010/00109] train_loss: 0.373\n",
      "Num batch val 68\n",
      "[010/00109] 2022-01-08 15:41:34.471713 val_loss: 0.423, val_accuracy: 86.227%\n",
      "[010/00159] train_loss: 0.369\n",
      "[010/00209] train_loss: 0.353\n",
      "Num batch val 68\n",
      "[010/00209] 2022-01-08 15:41:42.420585 val_loss: 0.672, val_accuracy: 79.295%\n",
      "[010/00259] train_loss: 0.349\n",
      "[010/00309] train_loss: 0.383\n",
      "Num batch val 68\n",
      "[010/00309] 2022-01-08 15:41:50.396431 val_loss: 0.516, val_accuracy: 84.557%\n",
      "[010/00359] train_loss: 0.352\n",
      "[010/00409] train_loss: 0.349\n",
      "Num batch val 68\n",
      "[010/00409] 2022-01-08 15:41:58.315947 val_loss: 0.346, val_accuracy: 89.156%\n",
      "[010/00459] train_loss: 0.372\n",
      "[011/00030] train_loss: 0.374\n",
      "Num batch val 68\n",
      "[011/00030] 2022-01-08 15:42:06.909867 val_loss: 0.504, val_accuracy: 85.061%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[011/00080] train_loss: 0.345\n",
      "[011/00130] train_loss: 0.355\n",
      "Num batch val 68\n",
      "[011/00130] 2022-01-08 15:42:14.596163 val_loss: 0.406, val_accuracy: 86.593%\n",
      "[011/00180] train_loss: 0.381\n",
      "[011/00230] train_loss: 0.359\n",
      "Num batch val 68\n",
      "[011/00230] 2022-01-08 15:42:22.298313 val_loss: 1.004, val_accuracy: 69.778%\n",
      "[011/00280] train_loss: 0.350\n",
      "[011/00330] train_loss: 0.370\n",
      "Num batch val 68\n",
      "[011/00330] 2022-01-08 15:42:30.093775 val_loss: 0.415, val_accuracy: 86.616%\n",
      "[011/00380] train_loss: 0.357\n",
      "[011/00430] train_loss: 0.393\n",
      "Num batch val 68\n",
      "[011/00430] 2022-01-08 15:42:37.816203 val_loss: 0.362, val_accuracy: 88.813%\n",
      "[012/00001] train_loss: 0.351\n",
      "[012/00051] train_loss: 0.351\n",
      "Num batch val 68\n",
      "[012/00051] 2022-01-08 15:42:46.026650 val_loss: 0.440, val_accuracy: 85.541%\n",
      "[012/00101] train_loss: 0.357\n",
      "[012/00151] train_loss: 0.357\n",
      "Num batch val 68\n",
      "[012/00151] 2022-01-08 15:42:53.867142 val_loss: 0.516, val_accuracy: 84.123%\n",
      "[012/00201] train_loss: 0.337\n",
      "[012/00251] train_loss: 0.364\n",
      "Num batch val 68\n",
      "[012/00251] 2022-01-08 15:43:01.766435 val_loss: 0.341, val_accuracy: 89.247%\n",
      "[012/00301] train_loss: 0.325\n",
      "[012/00351] train_loss: 0.364\n",
      "Num batch val 68\n",
      "[012/00351] 2022-01-08 15:43:09.781333 val_loss: 0.746, val_accuracy: 77.671%\n",
      "[012/00401] train_loss: 0.351\n",
      "[012/00451] train_loss: 0.380\n",
      "Num batch val 68\n",
      "[012/00451] 2022-01-08 15:43:17.555081 val_loss: 0.439, val_accuracy: 86.319%\n",
      "[013/00022] train_loss: 0.337\n",
      "[013/00072] train_loss: 0.343\n",
      "Num batch val 68\n",
      "[013/00072] 2022-01-08 15:43:25.890214 val_loss: 0.378, val_accuracy: 88.126%\n",
      "[013/00122] train_loss: 0.354\n",
      "[013/00172] train_loss: 0.361\n",
      "Num batch val 68\n",
      "[013/00172] 2022-01-08 15:43:33.826927 val_loss: 0.400, val_accuracy: 87.875%\n",
      "[013/00222] train_loss: 0.356\n",
      "[013/00272] train_loss: 0.367\n",
      "Num batch val 68\n",
      "[013/00272] 2022-01-08 15:43:41.662208 val_loss: 0.375, val_accuracy: 87.760%\n",
      "[013/00322] train_loss: 0.349\n",
      "[013/00372] train_loss: 0.334\n",
      "Num batch val 68\n",
      "[013/00372] 2022-01-08 15:43:49.498218 val_loss: 0.598, val_accuracy: 81.949%\n",
      "[013/00422] train_loss: 0.338\n",
      "[013/00472] train_loss: 0.347\n",
      "Num batch val 68\n",
      "[013/00472] 2022-01-08 15:43:57.391094 val_loss: 0.447, val_accuracy: 85.907%\n",
      "[014/00043] train_loss: 0.333\n",
      "[014/00093] train_loss: 0.370\n",
      "Num batch val 68\n",
      "[014/00093] 2022-01-08 15:44:05.694230 val_loss: 0.367, val_accuracy: 87.714%\n",
      "[014/00143] train_loss: 0.312\n",
      "[014/00193] train_loss: 0.379\n",
      "Num batch val 68\n",
      "[014/00193] 2022-01-08 15:44:13.406529 val_loss: 0.420, val_accuracy: 86.822%\n",
      "[014/00243] train_loss: 0.354\n",
      "[014/00293] train_loss: 0.376\n",
      "Num batch val 68\n",
      "[014/00293] 2022-01-08 15:44:21.164524 val_loss: 0.408, val_accuracy: 87.257%\n",
      "[014/00343] train_loss: 0.366\n",
      "[014/00393] train_loss: 0.341\n",
      "Num batch val 68\n",
      "[014/00393] 2022-01-08 15:44:28.914973 val_loss: 0.480, val_accuracy: 84.351%\n",
      "[014/00443] train_loss: 0.369\n",
      "[015/00014] train_loss: 0.341\n",
      "Num batch val 68\n",
      "[015/00014] 2022-01-08 15:44:37.334623 val_loss: 0.410, val_accuracy: 86.982%\n",
      "[015/00064] train_loss: 0.339\n",
      "[015/00114] train_loss: 0.362\n",
      "Num batch val 68\n",
      "[015/00114] 2022-01-08 15:44:45.106453 val_loss: 0.376, val_accuracy: 87.989%\n",
      "[015/00164] train_loss: 0.331\n",
      "[015/00214] train_loss: 0.369\n",
      "Num batch val 68\n",
      "[015/00214] 2022-01-08 15:44:52.889278 val_loss: 0.488, val_accuracy: 84.923%\n",
      "[015/00264] train_loss: 0.334\n",
      "[015/00314] train_loss: 0.347\n",
      "Num batch val 68\n",
      "[015/00314] 2022-01-08 15:45:00.724297 val_loss: 0.526, val_accuracy: 84.237%\n",
      "[015/00364] train_loss: 0.330\n",
      "[015/00414] train_loss: 0.355\n",
      "Num batch val 68\n",
      "[015/00414] 2022-01-08 15:45:08.770997 val_loss: 0.728, val_accuracy: 76.733%\n",
      "[015/00464] train_loss: 0.352\n",
      "[016/00035] train_loss: 0.331\n",
      "Num batch val 68\n",
      "[016/00035] 2022-01-08 15:45:17.229920 val_loss: 0.540, val_accuracy: 82.430%\n",
      "[016/00085] train_loss: 0.330\n",
      "[016/00135] train_loss: 0.361\n",
      "Num batch val 68\n",
      "[016/00135] 2022-01-08 15:45:24.908850 val_loss: 0.568, val_accuracy: 82.864%\n",
      "[016/00185] train_loss: 0.366\n",
      "[016/00235] train_loss: 0.330\n",
      "Num batch val 68\n",
      "[016/00235] 2022-01-08 15:45:32.578845 val_loss: 0.360, val_accuracy: 88.813%\n",
      "[016/00285] train_loss: 0.341\n",
      "[016/00335] train_loss: 0.333\n",
      "Num batch val 68\n",
      "[016/00335] 2022-01-08 15:45:40.338878 val_loss: 0.344, val_accuracy: 89.179%\n",
      "[016/00385] train_loss: 0.348\n",
      "[016/00435] train_loss: 0.325\n",
      "Num batch val 68\n",
      "[016/00435] 2022-01-08 15:45:48.074542 val_loss: 0.381, val_accuracy: 88.058%\n",
      "[017/00006] train_loss: 0.314\n",
      "[017/00056] train_loss: 0.327\n",
      "Num batch val 68\n",
      "[017/00056] 2022-01-08 15:45:56.385290 val_loss: 0.386, val_accuracy: 87.463%\n",
      "[017/00106] train_loss: 0.341\n",
      "[017/00156] train_loss: 0.338\n",
      "Num batch val 68\n",
      "[017/00156] 2022-01-08 15:46:04.183630 val_loss: 0.401, val_accuracy: 87.737%\n",
      "[017/00206] train_loss: 0.342\n",
      "[017/00256] train_loss: 0.353\n",
      "Num batch val 68\n",
      "[017/00256] 2022-01-08 15:46:12.028445 val_loss: 0.450, val_accuracy: 85.495%\n",
      "[017/00306] train_loss: 0.369\n",
      "[017/00356] train_loss: 0.309\n",
      "Num batch val 68\n",
      "[017/00356] 2022-01-08 15:46:19.996293 val_loss: 0.505, val_accuracy: 84.214%\n",
      "[017/00406] train_loss: 0.358\n",
      "[017/00456] train_loss: 0.347\n",
      "Num batch val 68\n",
      "[017/00456] 2022-01-08 15:46:27.829757 val_loss: 0.368, val_accuracy: 88.241%\n",
      "[018/00027] train_loss: 0.322\n",
      "[018/00077] train_loss: 0.354\n",
      "Num batch val 68\n",
      "[018/00077] 2022-01-08 15:46:36.106761 val_loss: 0.423, val_accuracy: 87.280%\n",
      "[018/00127] train_loss: 0.338\n",
      "[018/00177] train_loss: 0.335\n",
      "Num batch val 68\n",
      "[018/00177] 2022-01-08 15:46:43.901761 val_loss: 1.187, val_accuracy: 58.156%\n",
      "[018/00227] train_loss: 0.340\n",
      "[018/00277] train_loss: 0.352\n",
      "Num batch val 68\n",
      "[018/00277] 2022-01-08 15:46:51.875047 val_loss: 0.811, val_accuracy: 75.292%\n",
      "[018/00327] train_loss: 0.355\n",
      "[018/00377] train_loss: 0.339\n",
      "Num batch val 68\n",
      "[018/00377] 2022-01-08 15:46:59.799771 val_loss: 0.746, val_accuracy: 78.655%\n",
      "[018/00427] train_loss: 0.339\n",
      "[018/00477] train_loss: 0.325\n",
      "Num batch val 68\n",
      "[018/00477] 2022-01-08 15:47:07.558476 val_loss: 0.402, val_accuracy: 87.303%\n",
      "[019/00048] train_loss: 0.341\n",
      "[019/00098] train_loss: 0.345\n",
      "Num batch val 68\n",
      "[019/00098] 2022-01-08 15:47:15.946306 val_loss: 1.336, val_accuracy: 57.470%\n",
      "[019/00148] train_loss: 0.368\n",
      "[019/00198] train_loss: 0.334\n",
      "Num batch val 68\n",
      "[019/00198] 2022-01-08 15:47:23.862331 val_loss: 0.334, val_accuracy: 89.133%\n",
      "[019/00248] train_loss: 0.330\n",
      "[019/00298] train_loss: 0.353\n",
      "Num batch val 68\n",
      "[019/00298] 2022-01-08 15:47:31.716133 val_loss: 0.381, val_accuracy: 87.646%\n",
      "[019/00348] train_loss: 0.347\n",
      "[019/00398] train_loss: 0.336\n",
      "Num batch val 68\n",
      "[019/00398] 2022-01-08 15:47:39.683242 val_loss: 0.488, val_accuracy: 85.061%\n",
      "[019/00448] train_loss: 0.330\n",
      "[020/00019] train_loss: 0.324\n",
      "Num batch val 68\n",
      "[020/00019] 2022-01-08 15:47:48.052300 val_loss: 0.475, val_accuracy: 85.038%\n",
      "[020/00069] train_loss: 0.335\n",
      "[020/00119] train_loss: 0.343\n",
      "Num batch val 68\n",
      "[020/00119] 2022-01-08 15:47:55.972790 val_loss: 0.518, val_accuracy: 83.871%\n",
      "[020/00169] train_loss: 0.343\n",
      "[020/00219] train_loss: 0.333\n",
      "Num batch val 68\n",
      "[020/00219] 2022-01-08 15:48:03.672055 val_loss: 0.380, val_accuracy: 87.417%\n",
      "[020/00269] train_loss: 0.305\n",
      "[020/00319] train_loss: 0.318\n",
      "Num batch val 68\n",
      "[020/00319] 2022-01-08 15:48:11.593198 val_loss: 0.442, val_accuracy: 86.479%\n",
      "[020/00369] train_loss: 0.319\n",
      "[020/00419] train_loss: 0.379\n",
      "Num batch val 68\n",
      "[020/00419] 2022-01-08 15:48:19.356678 val_loss: 1.067, val_accuracy: 65.065%\n",
      "[020/00469] train_loss: 0.333\n",
      "[021/00040] train_loss: 0.308\n",
      "Num batch val 68\n",
      "[021/00040] 2022-01-08 15:48:27.662659 val_loss: 1.344, val_accuracy: 55.731%\n",
      "[021/00090] train_loss: 0.348\n",
      "[021/00140] train_loss: 0.335\n",
      "Num batch val 68\n",
      "[021/00140] 2022-01-08 15:48:35.590629 val_loss: 0.467, val_accuracy: 84.603%\n",
      "[021/00190] train_loss: 0.341\n",
      "[021/00240] train_loss: 0.347\n",
      "Num batch val 68\n",
      "[021/00240] 2022-01-08 15:48:43.457962 val_loss: 0.383, val_accuracy: 88.149%\n",
      "[021/00290] train_loss: 0.329\n",
      "[021/00340] train_loss: 0.340\n",
      "Num batch val 68\n",
      "[021/00340] 2022-01-08 15:48:51.405181 val_loss: 0.744, val_accuracy: 77.419%\n",
      "[021/00390] train_loss: 0.317\n",
      "[021/00440] train_loss: 0.372\n",
      "Num batch val 68\n",
      "[021/00440] 2022-01-08 15:48:59.317976 val_loss: 0.382, val_accuracy: 88.401%\n",
      "[022/00011] train_loss: 0.316\n",
      "[022/00061] train_loss: 0.298\n",
      "Num batch val 68\n",
      "[022/00061] 2022-01-08 15:49:07.598779 val_loss: 0.370, val_accuracy: 88.652%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[022/00111] train_loss: 0.345\n",
      "[022/00161] train_loss: 0.331\n",
      "Num batch val 68\n",
      "[022/00161] 2022-01-08 15:49:15.332624 val_loss: 0.356, val_accuracy: 88.858%\n",
      "[022/00211] train_loss: 0.330\n",
      "[022/00261] train_loss: 0.361\n",
      "Num batch val 68\n",
      "[022/00261] 2022-01-08 15:49:23.169175 val_loss: 0.576, val_accuracy: 81.560%\n",
      "[022/00311] train_loss: 0.341\n",
      "[022/00361] train_loss: 0.334\n",
      "Num batch val 68\n",
      "[022/00361] 2022-01-08 15:49:30.842597 val_loss: 0.355, val_accuracy: 88.698%\n",
      "[022/00411] train_loss: 0.351\n",
      "[022/00461] train_loss: 0.327\n",
      "Num batch val 68\n",
      "[022/00461] 2022-01-08 15:49:38.617373 val_loss: 0.514, val_accuracy: 83.574%\n",
      "[023/00032] train_loss: 0.335\n",
      "[023/00082] train_loss: 0.307\n",
      "Num batch val 68\n",
      "[023/00082] 2022-01-08 15:49:46.863200 val_loss: 0.671, val_accuracy: 78.289%\n",
      "[023/00132] train_loss: 0.316\n",
      "[023/00182] train_loss: 0.346\n",
      "Num batch val 68\n",
      "[023/00182] 2022-01-08 15:49:54.820217 val_loss: 0.407, val_accuracy: 87.303%\n",
      "[023/00232] train_loss: 0.333\n",
      "[023/00282] train_loss: 0.322\n",
      "Num batch val 68\n",
      "[023/00282] 2022-01-08 15:50:02.656920 val_loss: 0.357, val_accuracy: 89.041%\n",
      "[023/00332] train_loss: 0.327\n",
      "[023/00382] train_loss: 0.330\n",
      "Num batch val 68\n",
      "[023/00382] 2022-01-08 15:50:10.535776 val_loss: 0.337, val_accuracy: 89.407%\n",
      "[023/00432] train_loss: 0.324\n",
      "[024/00003] train_loss: 0.337\n",
      "Num batch val 68\n",
      "[024/00003] 2022-01-08 15:50:19.086870 val_loss: 0.501, val_accuracy: 83.825%\n",
      "[024/00053] train_loss: 0.326\n",
      "[024/00103] train_loss: 0.298\n",
      "Num batch val 68\n",
      "[024/00103] 2022-01-08 15:50:26.831541 val_loss: 0.406, val_accuracy: 87.348%\n",
      "[024/00153] train_loss: 0.326\n",
      "[024/00203] train_loss: 0.333\n",
      "Num batch val 68\n",
      "[024/00203] 2022-01-08 15:50:34.667300 val_loss: 0.434, val_accuracy: 87.165%\n",
      "[024/00253] train_loss: 0.318\n",
      "[024/00303] train_loss: 0.326\n",
      "Num batch val 68\n",
      "[024/00303] 2022-01-08 15:50:42.456152 val_loss: 0.772, val_accuracy: 75.818%\n",
      "[024/00353] train_loss: 0.346\n",
      "[024/00403] train_loss: 0.333\n",
      "Num batch val 68\n",
      "[024/00403] 2022-01-08 15:50:50.245941 val_loss: 0.345, val_accuracy: 89.247%\n",
      "[024/00453] train_loss: 0.326\n",
      "[025/00024] train_loss: 0.366\n",
      "Num batch val 68\n",
      "[025/00024] 2022-01-08 15:50:58.548048 val_loss: 0.451, val_accuracy: 85.999%\n",
      "[025/00074] train_loss: 0.322\n",
      "[025/00124] train_loss: 0.299\n",
      "Num batch val 68\n",
      "[025/00124] 2022-01-08 15:51:06.366982 val_loss: 0.392, val_accuracy: 87.669%\n",
      "[025/00174] train_loss: 0.332\n",
      "[025/00224] train_loss: 0.328\n",
      "Num batch val 68\n",
      "[025/00224] 2022-01-08 15:51:14.117760 val_loss: 0.646, val_accuracy: 79.844%\n",
      "[025/00274] train_loss: 0.338\n",
      "[025/00324] train_loss: 0.317\n",
      "Num batch val 68\n",
      "[025/00324] 2022-01-08 15:51:21.945200 val_loss: 0.725, val_accuracy: 76.573%\n",
      "[025/00374] train_loss: 0.322\n",
      "[025/00424] train_loss: 0.326\n",
      "Num batch val 68\n",
      "[025/00424] 2022-01-08 15:51:29.800739 val_loss: 0.362, val_accuracy: 88.355%\n",
      "[025/00474] train_loss: 0.337\n",
      "[026/00045] train_loss: 0.303\n",
      "Num batch val 68\n",
      "[026/00045] 2022-01-08 15:51:38.162884 val_loss: 0.382, val_accuracy: 88.012%\n",
      "[026/00095] train_loss: 0.330\n",
      "[026/00145] train_loss: 0.328\n",
      "Num batch val 68\n",
      "[026/00145] 2022-01-08 15:51:45.934734 val_loss: 0.348, val_accuracy: 89.064%\n",
      "[026/00195] train_loss: 0.327\n",
      "[026/00245] train_loss: 0.330\n",
      "Num batch val 68\n",
      "[026/00245] 2022-01-08 15:51:53.818146 val_loss: 0.353, val_accuracy: 88.767%\n",
      "[026/00295] train_loss: 0.323\n",
      "[026/00345] train_loss: 0.342\n",
      "Num batch val 68\n",
      "[026/00345] 2022-01-08 15:52:01.666336 val_loss: 0.340, val_accuracy: 89.522%\n",
      "[026/00395] train_loss: 0.322\n",
      "[026/00445] train_loss: 0.322\n",
      "Num batch val 68\n",
      "[026/00445] 2022-01-08 15:52:09.736090 val_loss: 0.329, val_accuracy: 89.682%\n",
      "[027/00016] train_loss: 0.332\n",
      "[027/00066] train_loss: 0.290\n",
      "Num batch val 68\n",
      "[027/00066] 2022-01-08 15:52:18.217108 val_loss: 0.788, val_accuracy: 74.262%\n",
      "[027/00116] train_loss: 0.315\n",
      "[027/00166] train_loss: 0.345\n",
      "Num batch val 68\n",
      "[027/00166] 2022-01-08 15:52:26.117924 val_loss: 0.357, val_accuracy: 88.927%\n",
      "[027/00216] train_loss: 0.345\n",
      "[027/00266] train_loss: 0.327\n",
      "Num batch val 68\n",
      "[027/00266] 2022-01-08 15:52:33.847172 val_loss: 0.350, val_accuracy: 88.652%\n",
      "[027/00316] train_loss: 0.319\n",
      "[027/00366] train_loss: 0.332\n",
      "Num batch val 68\n",
      "[027/00366] 2022-01-08 15:52:41.642549 val_loss: 0.402, val_accuracy: 87.600%\n",
      "[027/00416] train_loss: 0.337\n",
      "[027/00466] train_loss: 0.275\n",
      "Num batch val 68\n",
      "[027/00466] 2022-01-08 15:52:49.490413 val_loss: 0.349, val_accuracy: 88.721%\n",
      "[028/00037] train_loss: 0.322\n",
      "[028/00087] train_loss: 0.331\n",
      "Num batch val 68\n",
      "[028/00087] 2022-01-08 15:52:57.852643 val_loss: 0.341, val_accuracy: 89.156%\n",
      "[028/00137] train_loss: 0.337\n",
      "[028/00187] train_loss: 0.324\n",
      "Num batch val 68\n",
      "[028/00187] 2022-01-08 15:53:05.683529 val_loss: 0.572, val_accuracy: 82.041%\n",
      "[028/00237] train_loss: 0.336\n",
      "[028/00287] train_loss: 0.353\n",
      "Num batch val 68\n",
      "[028/00287] 2022-01-08 15:53:13.620226 val_loss: 0.639, val_accuracy: 80.210%\n",
      "[028/00337] train_loss: 0.339\n",
      "[028/00387] train_loss: 0.349\n",
      "Num batch val 68\n",
      "[028/00387] 2022-01-08 15:53:21.610651 val_loss: 0.395, val_accuracy: 87.417%\n",
      "[028/00437] train_loss: 0.321\n",
      "[029/00008] train_loss: 0.312\n",
      "Num batch val 68\n",
      "[029/00008] 2022-01-08 15:53:29.990471 val_loss: 0.347, val_accuracy: 89.019%\n",
      "[029/00058] train_loss: 0.340\n",
      "[029/00108] train_loss: 0.302\n",
      "Num batch val 68\n",
      "[029/00108] 2022-01-08 15:53:37.673335 val_loss: 0.335, val_accuracy: 89.247%\n",
      "[029/00158] train_loss: 0.317\n",
      "[029/00208] train_loss: 0.318\n",
      "Num batch val 68\n",
      "[029/00208] 2022-01-08 15:53:45.445684 val_loss: 0.523, val_accuracy: 83.299%\n",
      "[029/00258] train_loss: 0.317\n",
      "[029/00308] train_loss: 0.310\n",
      "Num batch val 68\n",
      "[029/00308] 2022-01-08 15:53:53.174174 val_loss: 0.415, val_accuracy: 87.371%\n",
      "[029/00358] train_loss: 0.332\n",
      "[029/00408] train_loss: 0.315\n",
      "Num batch val 68\n",
      "[029/00408] 2022-01-08 15:54:00.975054 val_loss: 0.357, val_accuracy: 88.607%\n",
      "[029/00458] train_loss: 0.346\n",
      "Best val_accuracy: 89.682%\n"
     ]
    }
   ],
   "source": [
    "### Fine-tune backbone from scratch\n",
    "from mvcnn_rec.training import train_mvcnn\n",
    "config = {\n",
    "    'experiment_name': '1vcnn_generalize_finetune_backbone_from_resnet',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 30,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 100,\n",
    "    'num_views': 1,\n",
    "    'random_start_view': True, \n",
    "    'freezee_backbone': False\n",
    "}\n",
    "train_mvcnn.main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5e3f4",
   "metadata": {},
   "source": [
    "#### After 30 epochs: val_acc ~89.7% => Finetune resnet give better model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe63aa",
   "metadata": {},
   "source": [
    "### (c) Training over the entire training set\n",
    "- First use backbone from finetune single views MVCNN\n",
    "- Then Finetune the backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c37da",
   "metadata": {},
   "source": [
    "#### Train MVCNN with freezing pretrained resnet18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da40144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 2.602\n",
      "[000/00099] train_loss: 2.468\n",
      "Num batch val 273\n",
      "[000/00099] 2022-01-08 15:20:51.280152 val_loss: 2.752, val_accuracy: 18.188%\n",
      "[000/00149] train_loss: 2.584\n",
      "[000/00199] train_loss: 2.545\n",
      "Num batch val 273\n",
      "[000/00199] 2022-01-08 15:21:12.398132 val_loss: 2.676, val_accuracy: 18.188%\n",
      "[000/00249] train_loss: 2.622\n",
      "[000/00299] train_loss: 2.580\n",
      "Num batch val 273\n",
      "[000/00299] 2022-01-08 15:21:33.352946 val_loss: 2.596, val_accuracy: 22.695%\n",
      "[000/00349] train_loss: 2.490\n",
      "[000/00399] train_loss: 2.623\n",
      "Num batch val 273\n",
      "[000/00399] 2022-01-08 15:21:54.513720 val_loss: 2.395, val_accuracy: 21.666%\n",
      "[000/00449] train_loss: 2.482\n",
      "[000/00499] train_loss: 2.577\n",
      "Num batch val 273\n",
      "[000/00499] 2022-01-08 15:22:15.410685 val_loss: 2.552, val_accuracy: 17.502%\n",
      "[000/00549] train_loss: 2.435\n",
      "[000/00599] train_loss: 2.484\n",
      "Num batch val 273\n",
      "[000/00599] 2022-01-08 15:22:36.563262 val_loss: 2.473, val_accuracy: 26.104%\n",
      "[000/00649] train_loss: 2.545\n",
      "[000/00699] train_loss: 2.617\n",
      "Num batch val 273\n",
      "[000/00699] 2022-01-08 15:22:57.792927 val_loss: 2.389, val_accuracy: 19.446%\n",
      "[000/00749] train_loss: 2.544\n",
      "[000/00799] train_loss: 2.479\n",
      "Num batch val 273\n",
      "[000/00799] 2022-01-08 15:23:18.785356 val_loss: 2.507, val_accuracy: 11.828%\n",
      "[000/00849] train_loss: 2.553\n",
      "[000/00899] train_loss: 2.479\n",
      "Num batch val 273\n",
      "[000/00899] 2022-01-08 15:23:39.830411 val_loss: 2.377, val_accuracy: 24.068%\n",
      "[000/00949] train_loss: 2.505\n",
      "[000/00999] train_loss: 2.487\n",
      "Num batch val 273\n",
      "[000/00999] 2022-01-08 15:24:00.848121 val_loss: 2.388, val_accuracy: 28.666%\n",
      "[000/01049] train_loss: 2.439\n",
      "[000/01099] train_loss: 2.444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1595186/102219339.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m'freezee_backbone'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# Set to False to finetune the backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m }\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_mvcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cluster/51/cuonghn/MVCNN_Reconstruction/mvcnn_rec/training/train_mvcnn.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cluster/51/cuonghn/MVCNN_Reconstruction/mvcnn_rec/training/train_mvcnn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, valloader, device, config)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/51/cuonghn/MVCNN_Reconstruction/mvcnn_rec/model/mvcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mclass_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 512, 1, 1 for (224 x 224) images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mclass_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mclass_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mclass_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-sN7A7CF0-py3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mvcnn_rec.training import train_mvcnn\n",
    "config = {\n",
    "    'experiment_name': 'mvcnn_generalize_8views_freezeebackbone',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 16,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 10,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 100,\n",
    "    'num_views': 8, # Num views from total 24 views (stride = 24 / num_views)\n",
    "    'random_start_view': True, # Set to False to get views start from idx 0\n",
    "    'freezee_backbone': True # Set to False to finetune the backbone\n",
    "}\n",
    "train_mvcnn.main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d442150",
   "metadata": {},
   "source": [
    "#### Train MVCNN and finetune resnet18 at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62aa71e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 2.474\n",
      "[000/00099] train_loss: 2.383\n",
      "Num batch val 273\n",
      "[000/00099] 2022-01-08 16:11:16.944299 val_loss: 2.403, val_accuracy: 19.355%\n",
      "[000/00149] train_loss: 2.367\n",
      "[000/00199] train_loss: 2.313\n",
      "Num batch val 273\n",
      "[000/00199] 2022-01-08 16:11:44.275021 val_loss: 2.361, val_accuracy: 15.488%\n",
      "[000/00249] train_loss: 2.386\n",
      "[000/00299] train_loss: 2.394\n",
      "Num batch val 273\n",
      "[000/00299] 2022-01-08 16:12:11.746318 val_loss: 2.462, val_accuracy: 17.136%\n",
      "[000/00349] train_loss: 2.393\n",
      "[000/00399] train_loss: 2.343\n",
      "Num batch val 273\n",
      "[000/00399] 2022-01-08 16:12:39.321615 val_loss: 2.360, val_accuracy: 15.488%\n",
      "[000/00449] train_loss: 2.349\n",
      "[000/00499] train_loss: 2.406\n",
      "Num batch val 273\n",
      "[000/00499] 2022-01-08 16:13:06.617697 val_loss: 2.369, val_accuracy: 17.136%\n",
      "[000/00549] train_loss: 2.351\n",
      "[000/00599] train_loss: 2.297\n",
      "Num batch val 273\n",
      "[000/00599] 2022-01-08 16:13:33.965425 val_loss: 2.356, val_accuracy: 19.446%\n",
      "[000/00649] train_loss: 2.370\n",
      "[000/00699] train_loss: 2.364\n",
      "Num batch val 273\n",
      "[000/00699] 2022-01-08 16:14:01.566524 val_loss: 2.364, val_accuracy: 17.136%\n",
      "[000/00749] train_loss: 2.393\n",
      "[000/00799] train_loss: 2.386\n",
      "Num batch val 273\n",
      "[000/00799] 2022-01-08 16:14:29.064703 val_loss: 2.394, val_accuracy: 19.446%\n",
      "[000/00849] train_loss: 2.336\n",
      "[000/00899] train_loss: 2.340\n",
      "Num batch val 273\n",
      "[000/00899] 2022-01-08 16:14:56.565955 val_loss: 2.341, val_accuracy: 17.136%\n",
      "[000/00949] train_loss: 2.379\n",
      "[000/00999] train_loss: 2.361\n",
      "Num batch val 273\n",
      "[000/00999] 2022-01-08 16:15:24.008934 val_loss: 2.368, val_accuracy: 15.488%\n",
      "[000/01049] train_loss: 2.346\n",
      "[000/01099] train_loss: 2.386\n",
      "Num batch val 273\n",
      "[000/01099] 2022-01-08 16:15:51.470893 val_loss: 2.360, val_accuracy: 17.136%\n",
      "[000/01149] train_loss: 2.362\n",
      "[000/01199] train_loss: 2.391\n",
      "Num batch val 273\n",
      "[000/01199] 2022-01-08 16:16:18.971899 val_loss: 2.393, val_accuracy: 17.136%\n",
      "[000/01249] train_loss: 2.414\n",
      "[000/01299] train_loss: 2.366\n",
      "Num batch val 273\n",
      "[000/01299] 2022-01-08 16:16:46.423000 val_loss: 2.362, val_accuracy: 17.136%\n",
      "[000/01349] train_loss: 2.403\n",
      "[000/01399] train_loss: 2.361\n",
      "Num batch val 273\n",
      "[000/01399] 2022-01-08 16:17:13.934157 val_loss: 2.366, val_accuracy: 19.446%\n",
      "[000/01449] train_loss: 2.355\n",
      "[000/01499] train_loss: 2.377\n",
      "Num batch val 273\n",
      "[000/01499] 2022-01-08 16:17:41.374631 val_loss: 2.384, val_accuracy: 19.401%\n",
      "[000/01549] train_loss: 2.373\n",
      "[000/01599] train_loss: 2.370\n",
      "Num batch val 273\n",
      "[000/01599] 2022-01-08 16:18:08.855931 val_loss: 2.354, val_accuracy: 19.446%\n",
      "[000/01649] train_loss: 2.365\n",
      "[000/01699] train_loss: 2.373\n",
      "Num batch val 273\n",
      "[000/01699] 2022-01-08 16:18:36.194457 val_loss: 2.374, val_accuracy: 17.136%\n",
      "[000/01749] train_loss: 2.346\n",
      "[000/01799] train_loss: 2.353\n",
      "Num batch val 273\n",
      "[000/01799] 2022-01-08 16:19:03.569725 val_loss: 2.347, val_accuracy: 17.136%\n",
      "[000/01849] train_loss: 2.336\n",
      "[000/01899] train_loss: 2.360\n",
      "Num batch val 273\n",
      "[000/01899] 2022-01-08 16:19:30.906957 val_loss: 2.351, val_accuracy: 18.943%\n",
      "[001/00033] train_loss: 2.365\n",
      "[001/00083] train_loss: 2.318\n",
      "Num batch val 273\n",
      "[001/00083] 2022-01-08 16:19:59.141898 val_loss: 2.352, val_accuracy: 19.446%\n",
      "[001/00133] train_loss: 2.377\n",
      "[001/00183] train_loss: 2.324\n",
      "Num batch val 273\n",
      "[001/00183] 2022-01-08 16:20:26.572566 val_loss: 2.343, val_accuracy: 17.136%\n",
      "[001/00233] train_loss: 2.366\n",
      "[001/00283] train_loss: 2.338\n",
      "Num batch val 273\n",
      "[001/00283] 2022-01-08 16:20:53.872433 val_loss: 2.352, val_accuracy: 17.136%\n",
      "[001/00333] train_loss: 2.324\n",
      "[001/00383] train_loss: 2.342\n",
      "Num batch val 273\n",
      "[001/00383] 2022-01-08 16:21:21.442769 val_loss: 5.789, val_accuracy: 19.446%\n",
      "[001/00433] train_loss: 2.365\n",
      "[001/00483] train_loss: 2.296\n",
      "Num batch val 273\n",
      "[001/00483] 2022-01-08 16:21:48.926957 val_loss: 2.348, val_accuracy: 17.136%\n",
      "[001/00533] train_loss: 2.373\n",
      "[001/00583] train_loss: 2.295\n",
      "Num batch val 273\n",
      "[001/00583] 2022-01-08 16:22:16.447073 val_loss: 2.360, val_accuracy: 19.446%\n",
      "[001/00633] train_loss: 2.357\n",
      "[001/00683] train_loss: 2.378\n",
      "Num batch val 273\n",
      "[001/00683] 2022-01-08 16:22:43.950929 val_loss: 2.322, val_accuracy: 19.195%\n",
      "[001/00733] train_loss: 2.336\n",
      "[001/00783] train_loss: 2.363\n",
      "Num batch val 273\n",
      "[001/00783] 2022-01-08 16:23:11.359837 val_loss: 2.366, val_accuracy: 17.136%\n",
      "[001/00833] train_loss: 2.377\n",
      "[001/00883] train_loss: 2.360\n",
      "Num batch val 273\n",
      "[001/00883] 2022-01-08 16:23:38.932826 val_loss: 2.351, val_accuracy: 17.136%\n",
      "[001/00933] train_loss: 2.337\n",
      "[001/00983] train_loss: 2.322\n",
      "Num batch val 273\n",
      "[001/00983] 2022-01-08 16:24:06.346600 val_loss: 2.331, val_accuracy: 19.446%\n",
      "[001/01033] train_loss: 2.330\n",
      "[001/01083] train_loss: 2.333\n",
      "Num batch val 273\n",
      "[001/01083] 2022-01-08 16:24:33.933182 val_loss: 2.339, val_accuracy: 19.446%\n",
      "[001/01133] train_loss: 2.356\n",
      "[001/01183] train_loss: 2.364\n",
      "Num batch val 273\n",
      "[001/01183] 2022-01-08 16:25:01.221888 val_loss: 2.332, val_accuracy: 19.446%\n",
      "[001/01233] train_loss: 2.386\n",
      "[001/01283] train_loss: 2.346\n",
      "Num batch val 273\n",
      "[001/01283] 2022-01-08 16:25:28.710389 val_loss: 2.338, val_accuracy: 19.446%\n",
      "[001/01333] train_loss: 2.350\n",
      "[001/01383] train_loss: 2.332\n",
      "Num batch val 273\n",
      "[001/01383] 2022-01-08 16:25:56.213142 val_loss: 2.338, val_accuracy: 19.446%\n",
      "[001/01433] train_loss: 2.313\n",
      "[001/01483] train_loss: 2.320\n",
      "Num batch val 273\n",
      "[001/01483] 2022-01-08 16:26:23.600637 val_loss: 2.334, val_accuracy: 15.488%\n",
      "[001/01533] train_loss: 2.354\n",
      "[001/01583] train_loss: 2.314\n",
      "Num batch val 273\n",
      "[001/01583] 2022-01-08 16:26:51.198874 val_loss: 2.337, val_accuracy: 19.446%\n",
      "[001/01633] train_loss: 2.297\n",
      "[001/01683] train_loss: 2.333\n",
      "Num batch val 273\n",
      "[001/01683] 2022-01-08 16:27:18.613710 val_loss: 2.335, val_accuracy: 17.479%\n",
      "[001/01733] train_loss: 2.374\n",
      "[001/01783] train_loss: 2.306\n",
      "Num batch val 273\n",
      "[001/01783] 2022-01-08 16:27:46.184217 val_loss: 2.332, val_accuracy: 19.446%\n",
      "[001/01833] train_loss: 2.362\n",
      "[001/01883] train_loss: 2.332\n",
      "Num batch val 273\n",
      "[001/01883] 2022-01-08 16:28:13.721913 val_loss: 2.334, val_accuracy: 19.446%\n",
      "[002/00017] train_loss: 2.305\n",
      "[002/00067] train_loss: 2.306\n",
      "Num batch val 273\n",
      "[002/00067] 2022-01-08 16:28:41.671494 val_loss: 2.335, val_accuracy: 19.446%\n",
      "[002/00117] train_loss: 2.328\n",
      "[002/00167] train_loss: 2.335\n",
      "Num batch val 273\n",
      "[002/00167] 2022-01-08 16:29:09.117402 val_loss: 2.334, val_accuracy: 19.446%\n",
      "[002/00217] train_loss: 2.321\n",
      "[002/00267] train_loss: 2.339\n",
      "Num batch val 273\n",
      "[002/00267] 2022-01-08 16:29:36.600703 val_loss: 2.334, val_accuracy: 19.446%\n",
      "[002/00317] train_loss: 2.352\n",
      "[002/00367] train_loss: 2.380\n",
      "Num batch val 273\n",
      "[002/00367] 2022-01-08 16:30:04.139818 val_loss: 2.335, val_accuracy: 19.446%\n",
      "[002/00417] train_loss: 2.305\n",
      "[002/00467] train_loss: 2.334\n",
      "Num batch val 273\n",
      "[002/00467] 2022-01-08 16:30:31.660317 val_loss: 2.330, val_accuracy: 19.446%\n",
      "[002/00517] train_loss: 2.319\n",
      "[002/00567] train_loss: 2.334\n",
      "Num batch val 273\n",
      "[002/00567] 2022-01-08 16:30:59.102156 val_loss: 2.333, val_accuracy: 17.136%\n",
      "[002/00617] train_loss: 2.370\n",
      "[002/00667] train_loss: 2.336\n",
      "Num batch val 273\n",
      "[002/00667] 2022-01-08 16:31:26.654072 val_loss: 2.332, val_accuracy: 19.446%\n",
      "[002/00717] train_loss: 2.335\n",
      "[002/00767] train_loss: 2.351\n",
      "Num batch val 273\n",
      "[002/00767] 2022-01-08 16:31:54.071065 val_loss: 2.331, val_accuracy: 19.446%\n",
      "[002/00817] train_loss: 2.334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1595186/2488165476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m'freezee_backbone'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# Set to False to finetune the backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m }\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_mvcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cluster/51/cuonghn/MVCNN_Reconstruction/mvcnn_rec/training/train_mvcnn.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cluster/51/cuonghn/MVCNN_Reconstruction/mvcnn_rec/training/train_mvcnn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, valloader, device, config)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# loss logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mtrain_loss_running\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mvcnn_rec.training import train_mvcnn\n",
    "config = {\n",
    "    'experiment_name': 'mvcnn_generalize_8views_finetune-resnet',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 16,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 10,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 100,\n",
    "    'num_views': 8, # Num views from total 24 views (stride = 24 / num_views)\n",
    "    'random_start_view': True, # Set to False to get views start from idx 0\n",
    "    'freezee_backbone': False # Set to False to finetune the backbone\n",
    "}\n",
    "train_mvcnn.main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24b7230",
   "metadata": {},
   "source": [
    "#### Train MVCNN with freezing finetuned (1VCNN) resnet18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c2361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 3.228\n",
      "[000/00099] train_loss: 2.765\n",
      "Num batch val 273\n",
      "[000/00099] 2022-01-08 16:32:28.901943 val_loss: 0.082, val_accuracy: 98.238%\n",
      "[000/00149] train_loss: 2.777\n",
      "[000/00199] train_loss: 2.682\n",
      "Num batch val 273\n",
      "[000/00199] 2022-01-08 16:32:50.164934 val_loss: 0.095, val_accuracy: 97.483%\n",
      "[000/00249] train_loss: 2.736\n",
      "[000/00299] train_loss: 2.664\n",
      "Num batch val 273\n",
      "[000/00299] 2022-01-08 16:33:11.265362 val_loss: 0.126, val_accuracy: 97.049%\n",
      "[000/00349] train_loss: 2.686\n",
      "[000/00399] train_loss: 2.720\n",
      "Num batch val 273\n",
      "[000/00399] 2022-01-08 16:33:32.332027 val_loss: 0.136, val_accuracy: 97.278%\n",
      "[000/00449] train_loss: 2.695\n",
      "[000/00499] train_loss: 2.718\n",
      "Num batch val 273\n",
      "[000/00499] 2022-01-08 16:33:53.131767 val_loss: 0.135, val_accuracy: 97.163%\n",
      "[000/00549] train_loss: 2.682\n",
      "[000/00599] train_loss: 2.716\n",
      "Num batch val 273\n",
      "[000/00599] 2022-01-08 16:34:14.277853 val_loss: 0.115, val_accuracy: 97.506%\n",
      "[000/00649] train_loss: 2.717\n",
      "[000/00699] train_loss: 2.659\n",
      "Num batch val 273\n",
      "[000/00699] 2022-01-08 16:34:35.360344 val_loss: 0.121, val_accuracy: 97.255%\n"
     ]
    }
   ],
   "source": [
    "from mvcnn_rec.training import train_mvcnn\n",
    "config = {\n",
    "    'experiment_name': 'mvcnn_generalize_8views_from_1view',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 16,\n",
    "    'resume_ckpt': 'mvcnn_rec/runs/1vcnn_generalize_finetune_backbone/model_best.ckpt',\n",
    "    'learning_rate': 0.0002,\n",
    "    'max_epochs': 1,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 100,\n",
    "    'num_views': 8, # Num views from total 24 views (stride = 24 / num_views)\n",
    "    'random_start_view': True, # Set to False to get views start from idx 0\n",
    "    'freezee_backbone': True # Set to False to finetune the backbone\n",
    "}\n",
    "train_mvcnn.main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb0e175",
   "metadata": {},
   "source": [
    "#### Verify very high val accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250efeb",
   "metadata": {},
   "source": [
    "### (d) Predict and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fed40e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvcnn_rec.inference.infer_mvcnn import InferenceHandlerMVCNN\n",
    "ckpt = 'mvcnn_rec/runs/mvcnn_generalize_8views_from_1view/model_best.ckpt'\n",
    "inferer = InferenceHandlerMVCNN(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "859c9ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4371"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = ShapeNetMultiview('val', total_views=24, num_views=8, \n",
    "                                load_mode='mvcnn_rec', # Change to mvcnn to get only images and labels\n",
    "                                random_start_view=True)\n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecd3979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 03001627/a1734a851af178bee15475f0b1eb22aa\n",
      "Images Dimensions: torch.Size([8, 3, 224, 224])\n",
      "Voxel Dimensions: (1, 32, 32, 32)\n",
      "Label: 4, Name: chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7270300c68947f5a76b3b81b15dc85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Random an data\n",
    "idx = np.random.randint(0, len(val_dataset))\n",
    "sample = val_dataset[idx]\n",
    "\n",
    "print(f'Name: {sample[\"name\"]}')\n",
    "print(f'Images Dimensions: {sample[\"item\"].shape}')\n",
    "print(f'Voxel Dimensions: {sample[\"voxel\"].shape}')\n",
    "print(f'Label: {sample[\"label\"]}, Name: {ShapeNetMultiview.id_class_mapping[sample[\"label\"]]}')\n",
    "\n",
    "visualize_occupancy(sample[\"voxel\"].squeeze(), flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6291a",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa7bc718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chair'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferer.infer_single(sample[\"item\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5128f",
   "metadata": {},
   "source": [
    "## TODO: 2. Multiview CNN for Classification and Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e046bcd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | Name                                     | Type                | Params  \n",
      "-------------------------------------------------------------------------------------\n",
      "0   | encoder_image                            | ResNet              | 11689512\n",
      "1   | encoder_image.conv1                      | Conv2d              | 9408    \n",
      "2   | encoder_image.bn1                        | BatchNorm2d         | 128     \n",
      "3   | encoder_image.relu                       | ReLU                | 0       \n",
      "4   | encoder_image.maxpool                    | MaxPool2d           | 0       \n",
      "5   | encoder_image.layer1                     | Sequential          | 147968  \n",
      "6   | encoder_image.layer1.0                   | BasicBlock          | 73984   \n",
      "7   | encoder_image.layer1.0.conv1             | Conv2d              | 36864   \n",
      "8   | encoder_image.layer1.0.bn1               | BatchNorm2d         | 128     \n",
      "9   | encoder_image.layer1.0.relu              | ReLU                | 0       \n",
      "10  | encoder_image.layer1.0.conv2             | Conv2d              | 36864   \n",
      "11  | encoder_image.layer1.0.bn2               | BatchNorm2d         | 128     \n",
      "12  | encoder_image.layer1.1                   | BasicBlock          | 73984   \n",
      "13  | encoder_image.layer1.1.conv1             | Conv2d              | 36864   \n",
      "14  | encoder_image.layer1.1.bn1               | BatchNorm2d         | 128     \n",
      "15  | encoder_image.layer1.1.relu              | ReLU                | 0       \n",
      "16  | encoder_image.layer1.1.conv2             | Conv2d              | 36864   \n",
      "17  | encoder_image.layer1.1.bn2               | BatchNorm2d         | 128     \n",
      "18  | encoder_image.layer2                     | Sequential          | 525568  \n",
      "19  | encoder_image.layer2.0                   | BasicBlock          | 230144  \n",
      "20  | encoder_image.layer2.0.conv1             | Conv2d              | 73728   \n",
      "21  | encoder_image.layer2.0.bn1               | BatchNorm2d         | 256     \n",
      "22  | encoder_image.layer2.0.relu              | ReLU                | 0       \n",
      "23  | encoder_image.layer2.0.conv2             | Conv2d              | 147456  \n",
      "24  | encoder_image.layer2.0.bn2               | BatchNorm2d         | 256     \n",
      "25  | encoder_image.layer2.0.downsample        | Sequential          | 8448    \n",
      "26  | encoder_image.layer2.0.downsample.0      | Conv2d              | 8192    \n",
      "27  | encoder_image.layer2.0.downsample.1      | BatchNorm2d         | 256     \n",
      "28  | encoder_image.layer2.1                   | BasicBlock          | 295424  \n",
      "29  | encoder_image.layer2.1.conv1             | Conv2d              | 147456  \n",
      "30  | encoder_image.layer2.1.bn1               | BatchNorm2d         | 256     \n",
      "31  | encoder_image.layer2.1.relu              | ReLU                | 0       \n",
      "32  | encoder_image.layer2.1.conv2             | Conv2d              | 147456  \n",
      "33  | encoder_image.layer2.1.bn2               | BatchNorm2d         | 256     \n",
      "34  | encoder_image.layer3                     | Sequential          | 2099712 \n",
      "35  | encoder_image.layer3.0                   | BasicBlock          | 919040  \n",
      "36  | encoder_image.layer3.0.conv1             | Conv2d              | 294912  \n",
      "37  | encoder_image.layer3.0.bn1               | BatchNorm2d         | 512     \n",
      "38  | encoder_image.layer3.0.relu              | ReLU                | 0       \n",
      "39  | encoder_image.layer3.0.conv2             | Conv2d              | 589824  \n",
      "40  | encoder_image.layer3.0.bn2               | BatchNorm2d         | 512     \n",
      "41  | encoder_image.layer3.0.downsample        | Sequential          | 33280   \n",
      "42  | encoder_image.layer3.0.downsample.0      | Conv2d              | 32768   \n",
      "43  | encoder_image.layer3.0.downsample.1      | BatchNorm2d         | 512     \n",
      "44  | encoder_image.layer3.1                   | BasicBlock          | 1180672 \n",
      "45  | encoder_image.layer3.1.conv1             | Conv2d              | 589824  \n",
      "46  | encoder_image.layer3.1.bn1               | BatchNorm2d         | 512     \n",
      "47  | encoder_image.layer3.1.relu              | ReLU                | 0       \n",
      "48  | encoder_image.layer3.1.conv2             | Conv2d              | 589824  \n",
      "49  | encoder_image.layer3.1.bn2               | BatchNorm2d         | 512     \n",
      "50  | encoder_image.layer4                     | Sequential          | 8393728 \n",
      "51  | encoder_image.layer4.0                   | BasicBlock          | 3673088 \n",
      "52  | encoder_image.layer4.0.conv1             | Conv2d              | 1179648 \n",
      "53  | encoder_image.layer4.0.bn1               | BatchNorm2d         | 1024    \n",
      "54  | encoder_image.layer4.0.relu              | ReLU                | 0       \n",
      "55  | encoder_image.layer4.0.conv2             | Conv2d              | 2359296 \n",
      "56  | encoder_image.layer4.0.bn2               | BatchNorm2d         | 1024    \n",
      "57  | encoder_image.layer4.0.downsample        | Sequential          | 132096  \n",
      "58  | encoder_image.layer4.0.downsample.0      | Conv2d              | 131072  \n",
      "59  | encoder_image.layer4.0.downsample.1      | BatchNorm2d         | 1024    \n",
      "60  | encoder_image.layer4.1                   | BasicBlock          | 4720640 \n",
      "61  | encoder_image.layer4.1.conv1             | Conv2d              | 2359296 \n",
      "62  | encoder_image.layer4.1.bn1               | BatchNorm2d         | 1024    \n",
      "63  | encoder_image.layer4.1.relu              | ReLU                | 0       \n",
      "64  | encoder_image.layer4.1.conv2             | Conv2d              | 2359296 \n",
      "65  | encoder_image.layer4.1.bn2               | BatchNorm2d         | 1024    \n",
      "66  | encoder_image.avgpool                    | AdaptiveAvgPool2d   | 0       \n",
      "67  | encoder_image.fc                         | Linear              | 513000  \n",
      "68  | reconstruction                           | MVCNNRec            | 4843242 \n",
      "69  | reconstruction.part_res                  | Sequential          | 683072  \n",
      "70  | reconstruction.part_res.0                | Conv2d              | 9408    \n",
      "71  | reconstruction.part_res.1                | BatchNorm2d         | 128     \n",
      "72  | reconstruction.part_res.2                | ReLU                | 0       \n",
      "73  | reconstruction.part_res.3                | MaxPool2d           | 0       \n",
      "74  | reconstruction.part_res.4                | Sequential          | 147968  \n",
      "75  | reconstruction.part_res.4.0              | BasicBlock          | 73984   \n",
      "76  | reconstruction.part_res.4.0.conv1        | Conv2d              | 36864   \n",
      "77  | reconstruction.part_res.4.0.bn1          | BatchNorm2d         | 128     \n",
      "78  | reconstruction.part_res.4.0.relu         | ReLU                | 0       \n",
      "79  | reconstruction.part_res.4.0.conv2        | Conv2d              | 36864   \n",
      "80  | reconstruction.part_res.4.0.bn2          | BatchNorm2d         | 128     \n",
      "81  | reconstruction.part_res.4.1              | BasicBlock          | 73984   \n",
      "82  | reconstruction.part_res.4.1.conv1        | Conv2d              | 36864   \n",
      "83  | reconstruction.part_res.4.1.bn1          | BatchNorm2d         | 128     \n",
      "84  | reconstruction.part_res.4.1.relu         | ReLU                | 0       \n",
      "85  | reconstruction.part_res.4.1.conv2        | Conv2d              | 36864   \n",
      "86  | reconstruction.part_res.4.1.bn2          | BatchNorm2d         | 128     \n",
      "87  | reconstruction.part_res.5                | Sequential          | 525568  \n",
      "88  | reconstruction.part_res.5.0              | BasicBlock          | 230144  \n",
      "89  | reconstruction.part_res.5.0.conv1        | Conv2d              | 73728   \n",
      "90  | reconstruction.part_res.5.0.bn1          | BatchNorm2d         | 256     \n",
      "91  | reconstruction.part_res.5.0.relu         | ReLU                | 0       \n",
      "92  | reconstruction.part_res.5.0.conv2        | Conv2d              | 147456  \n",
      "93  | reconstruction.part_res.5.0.bn2          | BatchNorm2d         | 256     \n",
      "94  | reconstruction.part_res.5.0.downsample   | Sequential          | 8448    \n",
      "95  | reconstruction.part_res.5.0.downsample.0 | Conv2d              | 8192    \n",
      "96  | reconstruction.part_res.5.0.downsample.1 | BatchNorm2d         | 256     \n",
      "97  | reconstruction.part_res.5.1              | BasicBlock          | 295424  \n",
      "98  | reconstruction.part_res.5.1.conv1        | Conv2d              | 147456  \n",
      "99  | reconstruction.part_res.5.1.bn1          | BatchNorm2d         | 256     \n",
      "100 | reconstruction.part_res.5.1.relu         | ReLU                | 0       \n",
      "101 | reconstruction.part_res.5.1.conv2        | Conv2d              | 147456  \n",
      "102 | reconstruction.part_res.5.1.bn2          | BatchNorm2d         | 256     \n",
      "103 | reconstruction.encoder                   | Sequential          | 258816  \n",
      "104 | reconstruction.encoder.0                 | Conv2d              | 147584  \n",
      "105 | reconstruction.encoder.1                 | BatchNorm2d         | 256     \n",
      "106 | reconstruction.encoder.2                 | ReLU                | 0       \n",
      "107 | reconstruction.encoder.3                 | Conv2d              | 73792   \n",
      "108 | reconstruction.encoder.4                 | BatchNorm2d         | 128     \n",
      "109 | reconstruction.encoder.5                 | ReLU                | 0       \n",
      "110 | reconstruction.encoder.6                 | MaxPool2d           | 0       \n",
      "111 | reconstruction.encoder.7                 | Conv2d              | 36928   \n",
      "112 | reconstruction.encoder.8                 | BatchNorm2d         | 128     \n",
      "113 | reconstruction.encoder.9                 | ReLU                | 0       \n",
      "114 | reconstruction.encoder.10                | MaxPool2d           | 0       \n",
      "115 | reconstruction.decoder                   | MVCNNDecoder        | 3883713 \n",
      "116 | reconstruction.decoder.model_1           | Sequential          | 3883704 \n",
      "117 | reconstruction.decoder.model_1.0         | ConvTranspose3d     | 3211392 \n",
      "118 | reconstruction.decoder.model_1.1         | BatchNorm3d         | 256     \n",
      "119 | reconstruction.decoder.model_1.2         | ReLU                | 0       \n",
      "120 | reconstruction.decoder.model_1.3         | ConvTranspose3d     | 524352  \n",
      "121 | reconstruction.decoder.model_1.4         | BatchNorm3d         | 128     \n",
      "122 | reconstruction.decoder.model_1.5         | ReLU                | 0       \n",
      "123 | reconstruction.decoder.model_1.6         | ConvTranspose3d     | 131104  \n",
      "124 | reconstruction.decoder.model_1.7         | BatchNorm3d         | 64      \n",
      "125 | reconstruction.decoder.model_1.8         | ReLU                | 0       \n",
      "126 | reconstruction.decoder.model_1.9         | ConvTranspose3d     | 16392   \n",
      "127 | reconstruction.decoder.model_1.10        | BatchNorm3d         | 16      \n",
      "128 | reconstruction.decoder.model_1.11        | ReLU                | 0       \n",
      "129 | reconstruction.decoder.model_2           | Sequential          | 9       \n",
      "130 | reconstruction.decoder.model_2.0         | ConvTranspose3d     | 9       \n",
      "131 | reconstruction.decoder.model_2.1         | Sigmoid             | 0       \n",
      "132 | reconstruction.csn                       | MVCNNCSN            | 17641   \n",
      "133 | reconstruction.csn.conv1                 | Sequential          | 2214    \n",
      "134 | reconstruction.csn.conv1.0               | Conv3d              | 2196    \n",
      "135 | reconstruction.csn.conv1.1               | BatchNorm3d         | 18      \n",
      "136 | reconstruction.csn.conv1.2               | LeakyReLU           | 0       \n",
      "137 | reconstruction.csn.conv2                 | Sequential          | 2214    \n",
      "138 | reconstruction.csn.conv2.0               | Conv3d              | 2196    \n",
      "139 | reconstruction.csn.conv2.1               | BatchNorm3d         | 18      \n",
      "140 | reconstruction.csn.conv2.2               | LeakyReLU           | 0       \n",
      "141 | reconstruction.csn.conv3                 | Sequential          | 2214    \n",
      "142 | reconstruction.csn.conv3.0               | Conv3d              | 2196    \n",
      "143 | reconstruction.csn.conv3.1               | BatchNorm3d         | 18      \n",
      "144 | reconstruction.csn.conv3.2               | LeakyReLU           | 0       \n",
      "145 | reconstruction.csn.conv4                 | Sequential          | 2214    \n",
      "146 | reconstruction.csn.conv4.0               | Conv3d              | 2196    \n",
      "147 | reconstruction.csn.conv4.1               | BatchNorm3d         | 18      \n",
      "148 | reconstruction.csn.conv4.2               | LeakyReLU           | 0       \n",
      "149 | reconstruction.csn.conv5                 | Sequential          | 8785    \n",
      "150 | reconstruction.csn.conv5.0               | Conv3d              | 8757    \n",
      "151 | reconstruction.csn.conv5.1               | BatchNorm3d         | 18      \n",
      "152 | reconstruction.csn.conv5.2               | LeakyReLU           | 0       \n",
      "153 | reconstruction.csn.conv5.3               | Conv3d              | 10      \n",
      "154 | reconstruction.csn.conv5.4               | LeakyReLU           | 0       \n",
      "155 | classifier                               | Sequential          | 1013012 \n",
      "156 | classifier.0                             | Linear              | 1001000 \n",
      "157 | classifier.1                             | ReLU                | 0       \n",
      "158 | classifier.2                             | Linear              | 12012   \n",
      "159 | TOTAL                                    | MVCNNReconstruction | 17545766\n"
     ]
    }
   ],
   "source": [
    "from mvcnn_rec.model.mvcnn_rec import MVCNNClassification, MVCNNReconstruction\n",
    "\n",
    "mvcnn_reconstruct = MVCNNReconstruction()\n",
    "print(summarize_model(mvcnn_reconstruct))  # Expected: Rows 0-71 and TOTAL = 12702524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e98d53d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mvcnn_reconstruct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1448539/2030057326.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmulti_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m222\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m222\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;31m# Suppose 24 images per shape, # 3 shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred_voxels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmvcnn_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output tensor shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_voxels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Expected: torch.Size(torch.Size([1, 32, 32, 32]) torch.Size([1, 12]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mvcnn_reconstruct' is not defined"
     ]
    }
   ],
   "source": [
    "multi_images = torch.randn(24, 5, 3, 222, 222) * 2. - 1. # Suppose 24 images per shape, # 3 shapes\n",
    "pred_voxels, pred_classes = mvcnn_reconstruct(multi_images)\n",
    "\n",
    "print('Output tensor shape: ', pred_voxels.shape, pred_classes.shape)  # Expected: torch.Size(torch.Size([1, 32, 32, 32]) torch.Size([1, 12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496e3ef",
   "metadata": {},
   "source": [
    "### (b) Training script and overfit one shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvcnn_rec.training import train_mvcnn_rec\n",
    "config = {\n",
    "    'experiment_name': 'mvcnn_rec_overfitting',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'batch_size': 4,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 20,\n",
    "    'print_every_n': 10,\n",
    "    'validate_every_n': 25,\n",
    "}\n",
    "train_mvcnn.main(config)  # should be able to get <0.0025 train_loss and <0.13 val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d77380",
   "metadata": {},
   "source": [
    "### (c) Training over the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a731af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvcnn_rec.training import train_mvcnn\n",
    "config = {\n",
    "    'experiment_name': 'mvcnn_rec_generalize',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'batch_size': 4,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 20,\n",
    "    'print_every_n': 10,\n",
    "    'validate_every_n': 25,\n",
    "}\n",
    "train_mvcnn.main(config)  # should be able to get <0.0025 train_loss and <0.13 val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cce5f8",
   "metadata": {},
   "source": [
    "### (d) Inference and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2a5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
